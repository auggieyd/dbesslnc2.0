{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create a database connection and Import data\n",
    "\n",
    "\n",
    "**modify the database table structure**\n",
    "```\n",
    "ALTER TABLE esslnc\n",
    "ADD CONSTRAINT unique_lnc \n",
    "UNIQUE (chr, start, end, strand);\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine,text\n",
    "\n",
    "\n",
    "username = 'root'\n",
    "password = 'root'\n",
    "host = 'localhost'  # general 'localhost'\n",
    "port = 3307  #Default MySQL port number.\n",
    "database = 'dbess2'\n",
    "\n",
    "\n",
    "engine = create_engine(f'mysql+pymysql://{username}:{password}@{host}:{port}/{database}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2: Import gene entries\n",
    "\n",
    "The files needed.\n",
    "1. /match/lncbook_map.tsv \n",
    "2. /match/noncode_map.tsv\n",
    "3. /match/gencode_map.tsv\n",
    "4. /match/non-coding_RNA.txt \n",
    "5. /match/go_map.txt\n",
    "6. /match/crispr_all.bed\n",
    "7. /match/merge.txt\n",
    "8. /clinvar_map/db/crispr_overlap/final_lncRNA_nocrispr.bed\n",
    "9. /cancer/unmap_from_dbesslnc.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2.1 : import lncRNA verified by CRISPR \n",
    " 1. main crispr lncRNA gene\n",
    " 2. update vitro column\n",
    " 3. Export data for variants mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. main crispr lncRNA gene\n",
    "merge_dict = {}\n",
    "with open('merge.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        if '=' in line:\n",
    "            gene1, gene2 = line.strip().split('=')\n",
    "            merge_dict[gene1] = gene2\n",
    "            merge_dict[gene2] = gene1\n",
    "\n",
    "df = pd.read_csv('crispr_all.bed', sep='\\t', header=None)\n",
    "\n",
    "df['gene_id'] = df[3].apply(lambda x: x.rsplit('-', 1)[0])\n",
    "\n",
    "\n",
    "merge_groups = {}\n",
    "processed_genes = set()\n",
    "\n",
    "for gene in df['gene_id'].unique():\n",
    "    if gene in processed_genes:\n",
    "        continue\n",
    "        \n",
    "    if gene in merge_dict:\n",
    "        partner = merge_dict[gene]\n",
    "        group_name = min(gene, partner) \n",
    "        merge_groups[gene] = group_name\n",
    "        merge_groups[partner] = group_name\n",
    "        processed_genes.add(gene)\n",
    "        processed_genes.add(partner)\n",
    "    else:\n",
    "        merge_groups[gene] = gene\n",
    "        processed_genes.add(gene)\n",
    "\n",
    "df['merge_group'] = df['gene_id'].map(merge_groups)\n",
    "\n",
    "\n",
    "result = df.groupby('merge_group').agg({\n",
    "    0: 'first',  # chr\n",
    "    1: lambda x: min(x) + 1,    # start\n",
    "    2: 'max',    # end\n",
    "    5: 'first',  # strand\n",
    "    'gene_id': lambda x: ';'.join(sorted(set(x)))  \n",
    "}).reset_index()\n",
    "\n",
    "result = result[[0, 1, 2, 5, 'gene_id']]  \n",
    "result.columns = ['chr', 'start', 'end', 'strand', 'target']  \n",
    "\n",
    "\n",
    "result.to_sql('esslnc', engine, if_exists='append', index=False)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. update vitro column\n",
    "with engine.connect() as conn:\n",
    "    update_sql = text(\"\"\"\n",
    "        UPDATE esslnc \n",
    "        SET vitro = 1\n",
    "    \"\"\")\n",
    "    conn.execute(update_sql)\n",
    "    conn.commit()\n",
    "\n",
    "print(\"Update successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2.2:Export data for variants mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Export data for variants mapping.\n",
    "query = \"\"\"\n",
    "SELECT chr,`start`,`end`,target,0,strand \n",
    "FROM esslnc\n",
    "WHERE vitro = 1\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "output_file = 'crispr.txt'\n",
    "df.to_csv(output_file, sep='\\t', index=False)\n",
    "\n",
    "print(f\"exported {len(df)} records {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2.3 import variants lncRNA gene to database,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.\n",
    "# Adjust the order of column names according to different files.\n",
    "\n",
    "input_file = 'final_lncRNA_nocrispr.bed'\n",
    "df = pd.read_csv(input_file, sep='\\t', header=None,\n",
    "                 names=['chr', 'start', 'end', 'Lncbook_id', 'Noncode_id', \n",
    "                       'strand', 'gene_name', 'NCBI_id', 'variants_num','disease_related'])\n",
    "print(len(df))\n",
    "\n",
    "df['Lncbook_id'] = df['Lncbook_id'].replace('-', 'N.A.')\n",
    "df['Noncode_id'] = df['Noncode_id'].replace('-', 'N.A.')\n",
    "df['NCBI_id'] = df['NCBI_id'].replace('-', 'N.A.')\n",
    "\n",
    "insert_data = df[['chr', 'start', 'end', 'Lncbook_id', 'Noncode_id', \n",
    "                  'strand', 'gene_name', 'NCBI_id','disease_related']]\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    for _, row in insert_data.iterrows():\n",
    "        try:\n",
    "            insert_sql = text(\"\"\"\n",
    "                INSERT IGNORE INTO esslnc \n",
    "                (chr, start, end, Lncbook_id, Noncode_id, strand, gene_name, NCBI_id,disease_related)\n",
    "                VALUES (:chr, :start, :end, :Lncbook_id, :Noncode_id, :strand, :gene_name, :NCBI_id,:disease_related)\n",
    "            \"\"\")\n",
    "            conn.execute(insert_sql, row.to_dict())\n",
    "        except Exception as e:\n",
    "            print(f\"skip\")\n",
    "            continue\n",
    "    conn.commit()\n",
    "\n",
    "\n",
    "# try:\n",
    "#     insert_data.to_sql('esslnc', engine, if_exists='append', index=False)\n",
    "# except Exception as e:\n",
    "#     print(f\"error {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Update disease_related column,Mark whether lncRNAs derived from CRISPR experiments are disease-related.\n",
    "with engine.connect() as conn:\n",
    "    update_sql = text(\"\"\"\n",
    "        UPDATE esslnc\n",
    "        SET disease_related = 1\n",
    "        WHERE vitro = 0\n",
    "    \"\"\")\n",
    "    conn.execute(update_sql)\n",
    "    conn.commit()\n",
    "\n",
    "print(\"Update successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.unmap_from_dbesslnc.txt,After manually searching the public database GeneCards, \n",
    "# additional annotation information was supplemented.\n",
    "input_file = 'unmap_from_dbesslnc.txt'\n",
    "df = pd.read_csv(input_file, sep='\\t')\n",
    "with engine.connect() as conn:\n",
    "    for _, row in df.iterrows():\n",
    "        try:\n",
    "            insert_sql = text(\"\"\"\n",
    "                INSERT IGNORE INTO esslnc \n",
    "                (chr, start, end, Lncbook_id, Noncode_id, strand, gene_name, NCBI_id,PMID)\n",
    "                VALUES (:chr, :start, :end, :Lncbook_id, :Noncode_id, :strand, :gene_name, :NCBI_id, :PMID)\n",
    "            \"\"\")\n",
    "            conn.execute(insert_sql, row.to_dict())\n",
    "        except Exception as e:\n",
    "            print(f\"skip\")\n",
    "            continue\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For lncRNAs from dbesslnc,mark the columns for cancer-related and in vivo.\n",
    "# dbesslnc_id.txt,The file contains the gene name and the corresponding role.\n",
    "df = pd.read_csv('dbesslnc_id.txt', sep='\\t')\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    general_genes = tuple(df[df['Role'] == 'General']['Name'].tolist())\n",
    "    suppressor_genes = tuple(df[df['Role'] == 'Tumor suppressor gene']['Name'].tolist())\n",
    "    oncogenes = tuple(df[df['Role'] == 'Oncogene']['Name'].tolist())\n",
    "\n",
    "    if general_genes:\n",
    "        update_general = text(\"\"\"\n",
    "            UPDATE esslnc \n",
    "            SET vivo = 1\n",
    "            WHERE gene_name IN :genes\n",
    "        \"\"\")\n",
    "        conn.execute(update_general, {'genes': general_genes})\n",
    "\n",
    "    if suppressor_genes:\n",
    "        update_suppressor = text(\"\"\"\n",
    "            UPDATE esslnc \n",
    "            SET cancer_related = 2\n",
    "            WHERE gene_name IN :genes\n",
    "        \"\"\")\n",
    "        conn.execute(update_suppressor, {'genes': suppressor_genes})\n",
    "\n",
    "    if oncogenes:\n",
    "        update_oncogene = text(\"\"\"\n",
    "            UPDATE esslnc \n",
    "            SET cancer_related = 1\n",
    "            WHERE gene_name IN :genes\n",
    "        \"\"\")\n",
    "        conn.execute(update_oncogene, {'genes': oncogenes})\n",
    "\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3: Import the mapped gene IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the mapped database gene IDs and gene names,along with other mapped data(lncbook/noncode/gencode_map.tsv,non-coding_RNA.txt,go_map.txt)\n",
    "\n",
    "# map_file = 'gencode_map.tsv' #*_map.tsv\n",
    "map_file = 'non-coding_RNA.txt' #*.txt\n",
    "# map_df = pd.read_csv(map_file, sep='\\t', header=None)\n",
    "map_df = pd.read_csv(map_file, sep='\\t')\n",
    "# map_dict = map_df[[0,4,5]].drop_duplicates()\n",
    "map_dict = map_df[['symbol', 'entrez_id']]\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    for _, row in map_dict.iterrows():\n",
    "        # Modify column names,\n",
    "        # if str(row[5]).startswith('ENSG'):\n",
    "        #     continue\n",
    "        # update_sql = text(\"\"\"\n",
    "        #     UPDATE esslnc \n",
    "        #     SET ensembl_id = :ensemnl \n",
    "        #     WHERE target LIKE :pattern1 \n",
    "        #     OR target LIKE :pattern2\n",
    "        # \"\"\")\n",
    "        # pattern1 = f\"{row[0]}%\"  \n",
    "        # pattern2 = f\"%;{row[0]}%\"  \n",
    "        # conn.execute(update_sql, {\n",
    "        #     \"entrez_id\": row[1],\n",
    "        #     \"pattern1\": pattern1,\n",
    "        #     \"pattern2\": pattern2\n",
    "        # })\n",
    "        # -------------------\n",
    "        # txt\n",
    "        update_sql = text(\"\"\"\n",
    "            UPDATE esslnc \n",
    "            SET NCBI_id = :entrez_id\n",
    "            WHERE gene_name = :symbol\n",
    "            \n",
    "        \"\"\")\n",
    "        \n",
    "        \n",
    "        conn.execute(update_sql, {\n",
    "            \"entrez_id\": 'N.A.' if pd.isna(row[1]) else row[1],\n",
    "            \"symbol\": row[0],\n",
    "        })\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4: Generating Unique Identifiers for lncRNA Entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Generating Unique Identifiers for lncRNA Entries\n",
    "query = \"\"\"\n",
    "SELECT num_id, chr, start \n",
    "FROM esslnc \n",
    "ORDER BY chr, start\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "df = pd.read_sql(query, engine)\n",
    "df['new_uid'] = ['ELH{:06d}'.format(i+1) for i in range(len(df))]\n",
    "\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    for index, row in df.iterrows():\n",
    "        update_sql = text(\"\"\"\n",
    "        UPDATE esslnc \n",
    "        SET UID = :new_uid \n",
    "        WHERE num_id = :num_id\n",
    "        \"\"\")\n",
    "        conn.execute(update_sql, {\"new_uid\": row['new_uid'], \"num_id\": row['num_id']})\n",
    "    conn.commit()\n",
    "\n",
    "print(f\"Successfully updated {len(df)} records。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step4.2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exported 1186 records crispr_mapping.tsv\n"
     ]
    }
   ],
   "source": [
    "# Create a local mapping table for crispr lncRNA transcript.\n",
    "query = \"\"\"\n",
    "SELECT UID, target \n",
    "FROM esslnc WHERE vitro = 1\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "\n",
    "def split_target(row):\n",
    "    targets = row['target'].split(';')\n",
    "    return [{'UID': row['UID'], 'target': target} for target in targets]\n",
    "\n",
    "\n",
    "expanded_rows = [item for _, row in df.iterrows() for item in split_target(row)]\n",
    "result_df = pd.DataFrame(expanded_rows)\n",
    "\n",
    "\n",
    "output_file = 'crispr_mapping.tsv'\n",
    "result_df.to_csv(output_file, sep='\\t', index=False)\n",
    "\n",
    "print(f\"exported {len(result_df)} records {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step4.3: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exported 1161 records crispr_UID.txt\n"
     ]
    }
   ],
   "source": [
    "# Create a local mapping table for variants lncRNA \n",
    "query = \"\"\"\n",
    "SELECT UID, target \n",
    "FROM esslnc WHERE vitro = 1\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "output_file = 'crispr_UID.txt'\n",
    "df.to_csv(output_file, sep='\\t', index=False)\n",
    "\n",
    "print(f\"exported {len(df)} records {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step4.4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export Create a local mapping table for disease_related lncRNA.\n",
    "query = text(\"\"\"\n",
    "    SELECT Lncbook_id,Noncode_id,UID\n",
    "    FROM esslnc \n",
    "    WHERE disease_related = 1 AND vitro = 0;\n",
    "\"\"\")\n",
    "\n",
    "output_file = 'disease_related_UID.txt'\n",
    "with engine.connect() as conn:\n",
    "\n",
    "    result = pd.read_sql(query, conn)\n",
    "    \n",
    "\n",
    "    result.to_csv(output_file, \n",
    "                  sep='\\t', \n",
    "                  index=False, \n",
    "                  header=True,\n",
    "                  na_rep='N.A.')\n",
    "\n",
    "print(f\"exported {len(result)} records {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step4.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exported 5119 records non_crispr_UID.txt\n"
     ]
    }
   ],
   "source": [
    "# export Create a local mapping table for non verified by crispr lncRNA.\n",
    "query = text(\"\"\"\n",
    "    SELECT Lncbook_id,Noncode_id,UID\n",
    "    FROM esslnc \n",
    "    WHERE  vitro = 0;\n",
    "\"\"\")\n",
    "\n",
    "output_file = 'non_crispr_UID.txt'\n",
    "with engine.connect() as conn:\n",
    "\n",
    "    result = pd.read_sql(query, conn)\n",
    "    \n",
    "\n",
    "    result.to_csv(output_file, \n",
    "                  sep='\\t', \n",
    "                  index=False, \n",
    "                  header=True,\n",
    "                  na_rep='N.A.')\n",
    "\n",
    "print(f\"exported {len(result)} records {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5: Import transcript Entries\n",
    "The required files:\n",
    "1. /match/seq_splice.bed seq_delete.bed seq_casrx.bed seq_crispri.bed\n",
    "2. /match/esslnc2.fa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully import 4635 records to trans table.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import Genomic location information.\n",
    "import pandas as pd\n",
    "\n",
    "# custum bed file, generated by gen_fa.ipynb step1, Just input different files as needed\n",
    "custum_bed_file = 'seq_casrx.bed'\n",
    "#transcript sequence crispr_gene.fa\n",
    "seq_file = 'lncRNAV2.fasta'\n",
    "\n",
    "mapping_df = pd.read_csv('crispr_mapping.tsv', sep='\\t',names=['UID','target'])\n",
    "target_to_uid = {}\n",
    "\n",
    "for _, row in mapping_df.iterrows():\n",
    "    target_to_uid[row['target']] = row['UID']\n",
    "\n",
    "df = pd.read_csv(custum_bed_file, sep='\\t', header=None, \n",
    "                 names=['chr','start','end','name','score','strand','block_starts','block_sizes'])\n",
    "\n",
    "\n",
    "def parse_fasta(fasta_file):\n",
    "    sequences = {}\n",
    "    current_seq_id = None\n",
    "    current_seq = []\n",
    "    \n",
    "    with open(fasta_file) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "\n",
    "                if current_seq_id:\n",
    "                    sequences[current_seq_id] = ''.join(current_seq)\n",
    "                current_seq_id = line[1:]  \n",
    "                current_seq = []\n",
    "            else:\n",
    "                current_seq.append(line)\n",
    "    if current_seq_id:\n",
    "        sequences[current_seq_id] = ''.join(current_seq)\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "\n",
    "transcript_fa = parse_fasta(seq_file)\n",
    "\n",
    "def process_transcript(row):\n",
    "\n",
    "    transcript_id = row['name'].rsplit('-',1)[1]\n",
    "    target = row['name'].rsplit('-',1)[0]\n",
    "    UID = target_to_uid.get(target)\n",
    "    fasta_seq = transcript_fa.get(transcript_id)\n",
    "    if fasta_seq is None:\n",
    "        FASTA = f\">{transcript_id}<br/>Sequence not found\"\n",
    "    else:\n",
    "        FASTA = f\">{transcript_id}<br/>{fasta_seq}\"\n",
    "\n",
    "    starts = [int(x) for x in row['block_starts'].split(',') if x]\n",
    "    sizes = [int(x) for x in row['block_sizes'].split(',') if x]\n",
    "    \n",
    "\n",
    "    exon_positions = []\n",
    "    for rel_start, size in zip(starts, sizes):\n",
    "        abs_start = row['start'] + rel_start + 1  # 1-base\n",
    "        abs_end = abs_start + size - 1\n",
    "        exon_positions.append(f\"{abs_start}-{abs_end}\")\n",
    "    \n",
    "    return {\n",
    "        'UID': UID,\n",
    "        'transcript_id': transcript_id,\n",
    "        'chr': row['chr'],\n",
    "        'start': row['start'] + 1,  # 1-base\n",
    "        'end': row['end'],\n",
    "        'length': sum(sizes),\n",
    "        'exon_num': len(sizes),\n",
    "        'exon_pos': ','.join(exon_positions),\n",
    "        'strand': row['strand'],\n",
    "        'FASTA':FASTA\n",
    "    }\n",
    "\n",
    "result_df = pd.DataFrame([process_transcript(row) for _, row in df.iterrows()])\n",
    "\n",
    "\n",
    "result_df.to_sql('trans', engine, if_exists='append', index=False)\n",
    "\n",
    "print(f\"Successfully import {len(result_df)} records to trans table.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step5.2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lncbook_id', 'NONCODE_Gene_ID', 'UID', 'NONCODE_TRANSCRIPT_ID', 'Lncbook_trans_id', 'chr', 'start', 'end', 'strand', 'exon_num', 'exon_pos', 'transcript_id']\n",
      "Total processed: 29772\n",
      "Successfully imported: 29772\n",
      "Skipped duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "# import non verified by crispr lncRNA \n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import text\n",
    "\n",
    "non_crispr_file = 'non_crispr_lncrna.csv'\n",
    "headers = pd.read_csv(non_crispr_file, nrows=0).columns.tolist()\n",
    "print(headers)\n",
    "\n",
    "sql = text(f\"\"\"\n",
    "    INSERT IGNORE INTO trans\n",
    "    ({', '.join(headers)})\n",
    "    VALUES ({', '.join([':' + col for col in headers])})\n",
    "\"\"\")\n",
    "\n",
    "total_rows = 0\n",
    "success_rows = 0\n",
    "\n",
    "for chunk in pd.read_csv(non_crispr_file, chunksize=1000):\n",
    "    with engine.connect() as conn:\n",
    "        for _, row in chunk.iterrows():\n",
    "            try:\n",
    "                result = conn.execute(sql, row.to_dict())\n",
    "                success_rows += result.rowcount\n",
    "                total_rows += 1\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "        conn.commit()\n",
    "print(f\"Total processed: {total_rows}\")\n",
    "print(f\"Successfully imported: {success_rows}\")\n",
    "print(f\"Skipped duplicates: {total_rows - success_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step5.3 Import sequence (lncRNA non verified by crispr)\n",
    "The required files(downloaded from lncbook v2.0 and Noncode V6.0)\n",
    "1. LncBookv2_OnlyLnc.fa\n",
    "2. outLncRNA.fa \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seq from lncbook/noncode\n",
    "import pandas as pd\n",
    "from sqlalchemy import text\n",
    "\n",
    "fa_file = 'LncBookv2_OnlyLnc.fa'\n",
    "\n",
    "sql_select = text(\"\"\"\n",
    "    SELECT transcript_id \n",
    "    FROM trans \n",
    "    WHERE FASTA IS NULL\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "fasta_dict = {}\n",
    "with open(fa_file, 'r') as f:\n",
    "    seq = ''\n",
    "    tid = ''\n",
    "    for line in f:\n",
    "        if line.startswith('>'):\n",
    "            if tid and seq:\n",
    "                fasta_dict[tid] = seq\n",
    "            tid = line.strip().lstrip('>')\n",
    "            seq = ''\n",
    "        else:\n",
    "            seq += line.strip()\n",
    "    if tid and seq:\n",
    "        fasta_dict[tid] = seq\n",
    "\n",
    "\n",
    "sql_update = text(\"\"\"\n",
    "    UPDATE trans \n",
    "    SET FASTA = :fasta \n",
    "    WHERE transcript_id = :tid\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    transcript_ids = conn.execute(sql_select).fetchall()\n",
    "\n",
    "    for (tid,) in transcript_ids:\n",
    "        if tid in fasta_dict:\n",
    "            fasta_content = f\"{tid}<br/>{fasta_dict[tid]}\"\n",
    "            try:\n",
    "                conn.execute(sql_update, {\"fasta\": fasta_content, \"tid\": tid})\n",
    "            except Exception as e:\n",
    "                print(f\"Error updating {tid}: {e}\")\n",
    "    \n",
    "    conn.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6: Import the mapped transcript IDs\n",
    "The required files\n",
    "1. /match/lncbook_map.tsv\n",
    "2. /match/noncode_map.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the mapped database gene IDs and transcript IDs,along with other mapped data.\n",
    "# You can modify SQL statements and input files to update fields in the database.\n",
    "map_file = 'lncbook_map.tsv' #*_map.tsv,\n",
    "\n",
    "map_df = pd.read_csv(map_file, sep='\\t', header=None)\n",
    "# filter\n",
    "map_dict = map_df[map_df[6] == 'transcript'][[1,3,4]].drop_duplicates()\n",
    "map_dict.columns = ['transcript_id','Lncbook_trans_id', 'Lncbook_id'] # You can modify the corresponding column names.\n",
    "# print(map_dict.head())\n",
    "\n",
    "\n",
    "for _, lnc_row in map_dict.iterrows():\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            update_sql = text(\"\"\"\n",
    "            UPDATE trans \n",
    "            SET Lncbook_trans_id = :Lncbook_trans_id, \n",
    "                Lncbook_id = :Lncbook_id\n",
    "            WHERE transcript_id = :transcript_id\n",
    "            \"\"\")\n",
    "            \n",
    "            result = conn.execute(update_sql, {\n",
    "                \"transcript_id\": lnc_row['transcript_id'],\n",
    "                \"Lncbook_id\": lnc_row['Lncbook_id'],\n",
    "                \"Lncbook_trans_id\": lnc_row['Lncbook_trans_id']\n",
    "            })\n",
    "            \n",
    "            rows_affected = result.rowcount\n",
    "            # print(f\"update records: {lnc_row['transcript_id']}, affected rows: {rows_affected}\")\n",
    "            \n",
    "            conn.commit()\n",
    "    except Exception as e:\n",
    "        print(f\"update fail: {lnc_row['transcript_id']}, error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step7: import CRISPR experiment record `/curated/exp_crispr.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "exp_df = pd.read_csv('../store/exp_crispr.csv')\n",
    "\n",
    "exp_df.to_sql('exp_crispr', \n",
    "              engine, \n",
    "              if_exists='append', \n",
    "              index=False,         \n",
    "              chunksize=1000)      \n",
    "\n",
    "print(f\"Successfully import {len(exp_df)} records to trans table.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step7.1 \n",
    "Group by   exp_type   and   target_id   columns, for groups with a count of 1, mark as 'cell-line specific' and update in the table; for groups with a count of 2-5, mark as 'common essential'; and for groups with a count greater than 5, mark as 'core essential'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import text\n",
    "\n",
    "\n",
    "sql_select = text(\"\"\"\n",
    "    SELECT exp_type, target_id \n",
    "    FROM exp_crispr\n",
    "\"\"\")\n",
    "\n",
    "with engine.connect() as conn:\n",
    "\n",
    "    df = pd.read_sql(sql_select, conn)\n",
    "    \n",
    "\n",
    "    group_counts = df.groupby(['exp_type', 'target_id']).size()\n",
    "    \n",
    "\n",
    "    cell_specific = group_counts[group_counts == 1].index\n",
    "    common = group_counts[(group_counts >= 2) & (group_counts <= 5)].index\n",
    "    core = group_counts[group_counts > 5].index\n",
    "    \n",
    "\n",
    "    update_sql = text(\"\"\"\n",
    "        UPDATE exp_crispr\n",
    "        SET role = :etype \n",
    "        WHERE exp_type = :exp AND target_id = :tid\n",
    "    \"\"\")\n",
    "    \n",
    "\n",
    "    for exp, tid in cell_specific:\n",
    "        conn.execute(update_sql, {\"etype\": \"cell-line specific\", \"exp\": exp, \"tid\": tid})\n",
    "        \n",
    "    for exp, tid in common:\n",
    "        conn.execute(update_sql, {\"etype\": \"common essential\", \"exp\": exp, \"tid\": tid})\n",
    "        \n",
    "    for exp, tid in core:\n",
    "        conn.execute(update_sql, {\"etype\": \"core essential\", \"exp\": exp, \"tid\": tid})\n",
    "    \n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step8：import variants Mapping table and variants\n",
    "The required files.\n",
    "1. /clinvar_map/db/construct_map/disease_mapping.csv\n",
    "2. /clinvar_map/db/construct_map/crispr_mapping.csv\n",
    "3. /clinvar_map/db/crispr_overlap/variants_nocrispr.csv\n",
    "4. /clinvar_map/db/crispr_map/crispr_variants.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import variant information and mapping tables into the database.\n",
    "import pandas as pd\n",
    "from sqlalchemy import text\n",
    "\n",
    "variants_file = 'disease_mapping.csv'\n",
    "headers = pd.read_csv(variants_file, nrows=0).columns.tolist()\n",
    "\n",
    "#table name variants/lncrna_variant_mapping\n",
    "sql = text(f\"\"\"\n",
    "    INSERT IGNORE INTO lncrna_variant_mapping\n",
    "    ({', '.join(headers)})\n",
    "    VALUES ({', '.join([':' + col for col in headers])})\n",
    "\"\"\")\n",
    "\n",
    "total_rows = 0\n",
    "success_rows = 0\n",
    "\n",
    "for chunk in pd.read_csv(variants_file, chunksize=1000):\n",
    "    with engine.connect() as conn:\n",
    "        for _, row in chunk.iterrows():\n",
    "            try:\n",
    "                result = conn.execute(sql, row.to_dict())\n",
    "                success_rows += result.rowcount\n",
    "                total_rows += 1\n",
    "            except Exception as e:\n",
    "                continue\n",
    "        conn.commit()\n",
    "print(f\"Total processed: {total_rows}\")\n",
    "print(f\"Successfully imported: {success_rows}\")\n",
    "print(f\"Skipped duplicates: {total_rows - success_rows}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step9: import expression profile\n",
    "The required file\n",
    "1. /clinvar_map/db/exp/exp_profile.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UID', 'Lncbook_trans_id', 'transcript_id', 'brain', 'lung', 'urinarybladder', 'kidney', 'adrenal', 'thyroid', 'heart', 'lymphnode', 'spleen', 'bonemarrow', 'tonsil', 'appendix', 'colon', 'esophagus', 'gallbladder', 'smallintestine', 'salivarygland', 'stomach', 'liver', 'duodenum', 'pancreas', 'rectum', 'endometrium', 'ovary', 'testis', 'prostate', 'fallopiantube', 'skeletalmuscle', 'smoothmuscle', 'skin', 'fat']\n",
      "Total processed: 26642\n",
      "Successfully imported: 26642\n",
      "Skipped duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "# import all lncRNA expression profile\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import text\n",
    "\n",
    "expression_file = 'exp_profile.csv'\n",
    "headers = pd.read_csv(expression_file, nrows=0).columns.tolist()\n",
    "print(headers)\n",
    "\n",
    "sql = text(f\"\"\"\n",
    "    INSERT IGNORE INTO exp_profile\n",
    "    ({', '.join(headers)})\n",
    "    VALUES ({', '.join([':' + col for col in headers])})\n",
    "\"\"\")\n",
    "\n",
    "total_rows = 0\n",
    "success_rows = 0\n",
    "\n",
    "for chunk in pd.read_csv(expression_file, chunksize=1000):\n",
    "    with engine.connect() as conn:\n",
    "        for _, row in chunk.iterrows():\n",
    "            try:\n",
    "                result = conn.execute(sql, row.to_dict())\n",
    "                success_rows += result.rowcount\n",
    "                total_rows += 1\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "        conn.commit()\n",
    "print(f\"Total processed: {total_rows}\")\n",
    "print(f\"Successfully imported: {success_rows}\")\n",
    "print(f\"Skipped duplicates: {total_rows - success_rows}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
