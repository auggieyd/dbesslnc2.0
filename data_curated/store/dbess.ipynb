{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create a database connection and Import data\n",
    "\n",
    "\n",
    "**Preparations before executing the code**\n",
    "```\n",
    "## Create a database\n",
    "create database ess_test;\n",
    "use ess_test;\n",
    "\n",
    "## Import table structure\n",
    "source /your_path/store/dbess_schema.sql;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine,text\n",
    "\n",
    "\n",
    "username = 'root'\n",
    "password = 'root'\n",
    "host = 'localhost'  # general 'localhost'\n",
    "port = 3307  #Default MySQL port number.\n",
    "database = 'ess_test'\n",
    "\n",
    "\n",
    "engine = create_engine(f'mysql+pymysql://{username}:{password}@{host}:{port}/{database}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2: Import gene entries\n",
    "\n",
    "The files needed.\n",
    "1. /match/test/map/lncbook_map.tsv\n",
    "2. /match/test/map/noncode_map.tsv\n",
    "3. /match/test/map/gencode_map.tsv\n",
    "4. /match/test/map/ncbi_map.tsv\n",
    "5. /match/crispr_all.bed\n",
    "6. /cancer/unmap_from_dbesslnc.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2.1 : import lncRNA verified by CRISPR \n",
    " 1. main crispr lncRNA gene\n",
    " 2. update vitro column\n",
    " 3. Export data for variants mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_path = '../match/'\n",
    "crispr_bed_file = relative_path + 'test/output/crispr_all_import.bed'\n",
    "\n",
    "df = pd.read_csv(crispr_bed_file, sep='\\t', header=None)\n",
    "\n",
    "df['gene_id'] = df[3].apply(lambda x: x.rsplit('-', 1)[0])\n",
    "\n",
    "# Directly use the gene_id as the grouping basis without merging.\n",
    "df['merge_group'] = df['gene_id'] + '_' + df[6].astype(str)\n",
    "\n",
    "# group by gene_id and merge records\n",
    "result = df.groupby('merge_group').agg({\n",
    "    0: 'first',  # chr\n",
    "    1: lambda x: min(x) + 1,    # start\n",
    "    2: 'max',    # end\n",
    "    5: 'first',  # strand\n",
    "    'gene_id': lambda x: ';'.join(sorted(set(x)))  \n",
    "}).reset_index()\n",
    "\n",
    "result = result[[0, 1, 2, 5, 'gene_id']]  \n",
    "result.columns = ['chr', 'start', 'end', 'strand', 'target']  \n",
    "print(len(result))\n",
    "\n",
    "result.to_sql('esslnc', engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert in the manually retrieved supplementary lncRNA gene entries\n",
    "data = [\n",
    "        {'Noncode_id': 'N.A.', 'Lncbook_id': 'N.A.', 'ensembl_id': 'N.A.', \n",
    "         'target': 'LH00477', 'chr': 'chr1', 'start': 145410838, \n",
    "         'end': 145413269, 'strand': '+'},\n",
    "        {'Noncode_id': 'NONHSAG005780.3', 'Lncbook_id': 'HSALNG0077928', \n",
    "         'ensembl_id': 'ENSG000000290921.2', 'target': 'LH02126', \n",
    "         'chr': 'chr10', 'start': 45888164, 'end': 45972422, 'strand': '+'},\n",
    "        {'Noncode_id': 'NONHSAG097226.1', 'Lncbook_id': 'HSALNG0060060', \n",
    "         'ensembl_id': 'N.A.', 'target': 'LH14878', 'chr': 'chr7', \n",
    "         'start': 100963828, 'end': 100968124, 'strand': '-'}\n",
    "    ]\n",
    "    \n",
    "    # 2. Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.to_sql('esslnc', engine, if_exists='append', index=False)\n",
    "print(f\"Successfully inserted {len(df)} supplementary lncRNA gene entries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. update vitro and Organism column\n",
    "with engine.connect() as conn:\n",
    "    update_sql = text(\"\"\"\n",
    "        UPDATE esslnc \n",
    "        SET vitro = 1,Organism = 'Human'\n",
    "    \"\"\")\n",
    "    conn.execute(update_sql)\n",
    "    conn.commit()\n",
    "\n",
    "print(\"Update successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the externally mapped database ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the mapped database gene IDs and gene names,along with other mapped data(lncbook/ncbi/noncode/gencode_map.tsv)\n",
    "\n",
    "# for lncbook and noncode IDs\n",
    "def import_mapped_data(map_file,db):\n",
    "    # for lncbook and noncode IDs\n",
    "    map_df = pd.read_csv(map_file, sep='\\t', header=None)\n",
    "    map_dict = map_df[[0,3,4]].drop_duplicates()\n",
    "    with engine.connect() as conn:\n",
    "        for _, row in map_dict.iterrows():\n",
    "            if db != 'Noncode_id':\n",
    "                update_sql_name = text(f\"\"\"\n",
    "                    UPDATE esslnc \n",
    "                    SET gene_name = :gene_name\n",
    "                    WHERE target LIKE :pattern1 \n",
    "                    OR target LIKE :pattern2\n",
    "                    AND (gene_name IS NULL or gene_name = 'N.A.') AND num_id > 2712\n",
    "                \"\"\")\n",
    "\n",
    "            update_sql = text(f\"\"\"\n",
    "                UPDATE esslnc \n",
    "                SET {db} = :id\n",
    "                WHERE target LIKE :pattern1 \n",
    "                OR target LIKE :pattern2 AND num_id > 2712\n",
    "            \"\"\")\n",
    "            pattern1 = f\"{row[0]}%\"  \n",
    "            pattern2 = f\"%;{row[0]}%\"  \n",
    "            gene_name = 'N.A.' if str(row[4]).startswith('ENSG') or str(row[4]).startswith('LOC') else row[4]\n",
    "            conn.execute(update_sql, {\n",
    "                \"id\": row[3], \n",
    "                \"pattern1\": pattern1,\n",
    "                \"pattern2\": pattern2\n",
    "            })\n",
    "            if db != 'Noncode_id':\n",
    "                conn.execute(update_sql_name, {\n",
    "                    \"gene_name\": gene_name,\n",
    "                    \"pattern1\": pattern1,\n",
    "                    \"pattern2\": pattern2\n",
    "                })\n",
    "            conn.commit()\n",
    "\n",
    "import_mapped_data('../match/test/map/ncbi_map.tsv', 'NCBI_id')\n",
    "import_mapped_data('../match/test/map/gencode_map.tsv', 'ensembl_id')\n",
    "import_mapped_data('../match/test/map/lncbook_map.tsv', 'Lncbook_id')\n",
    "import_mapped_data('../match/test/map/noncode_map.tsv', 'Noncode_id')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the reason summary of genes verified by CRISPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read exp_crispr.csv file\n",
    "df = pd.read_csv('../curated/exp_crispr.csv')\n",
    "\n",
    "# Group by target_id and collect all exp_type\n",
    "target_summary = df.groupby(['target_id','PMID'],as_index=False).agg({\n",
    "    'exp_type': lambda x: ', '.join(sorted(set(x))),\n",
    "})\n",
    "\n",
    "# Update database\n",
    "with engine.connect() as conn:\n",
    "    for _, row in target_summary.iterrows():\n",
    "        target_id = row['target_id']\n",
    "        exp_types = row['exp_type']\n",
    "        pmids = row['PMID']\n",
    "        reason_summary = f\"[{exp_types}] Verified by the {exp_types} experiment. \"\n",
    "        \n",
    "        update_sql = text(\"\"\"\n",
    "            UPDATE esslnc \n",
    "            SET reason_summary = :reason_summary,\n",
    "            PMID = :pmids\n",
    "            WHERE target = :target_id and reason_summary IS NULL\n",
    "            Limit 1\n",
    "        \"\"\")\n",
    "        \n",
    "        conn.execute(update_sql, {\n",
    "            \"reason_summary\": reason_summary,\n",
    "            \"pmids\": pmids,\n",
    "            \"target_id\": target_id\n",
    "        })\n",
    "    \n",
    "    conn.commit()\n",
    "\n",
    "print(f\"Successfully updated {len(target_summary)} genes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2.2 import variants lncRNA gene to database,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.\n",
    "# Adjust the order of column names according to different files.\n",
    "\n",
    "input_file = '../clinvar_map/db/construct_map/unique_lncRNA.csv'\n",
    "df = pd.read_csv(input_file, sep=',', header=0,\n",
    "                 names=['Lncbook_id', 'Noncode_id', 'gene_name', 'NCBI_id', 'variants_num','chr', 'start', 'end', 'strand','source'])\n",
    "print(len(df))\n",
    "# Remove trailing '.0' from NCBI_id column if present\n",
    "\n",
    "df[['Lncbook_id', 'Noncode_id', 'gene_name', 'NCBI_id']] = df[['Lncbook_id', 'Noncode_id', 'gene_name', 'NCBI_id']].fillna('N.A.').replace('', 'N.A.')\n",
    "\n",
    "# Replace empty strings and NaN values with 'N.A.' for all relevant columns\n",
    "df['NCBI_id'] = df['NCBI_id'].astype(str).str.replace(r'\\.0$', '', regex=True)\n",
    "insert_data = df[['chr', 'start', 'end', 'Lncbook_id', 'Noncode_id', \n",
    "                  'strand', 'gene_name', 'NCBI_id']]\n",
    "print(insert_data.head(10))\n",
    "print(len(insert_data))\n",
    "with engine.connect() as conn:\n",
    "    for _, row in insert_data.iterrows():\n",
    "        try:\n",
    "            insert_sql = text(\"\"\"\n",
    "                INSERT IGNORE INTO esslnc \n",
    "                (chr, start, end, Lncbook_id, Noncode_id, strand, gene_name, NCBI_id)\n",
    "                VALUES (:chr, :start, :end, :Lncbook_id, :Noncode_id, :strand, :gene_name, :NCBI_id)\n",
    "            \"\"\")\n",
    "            conn.execute(insert_sql, row.to_dict())\n",
    "        except Exception as e:\n",
    "            print(f\"error {str(e)}\")\n",
    "            print(f\"skip\")\n",
    "            continue\n",
    "    conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Update disease_related column,Mark whether lncRNAs derived from CRISPR experiments are disease-related.\n",
    "# Supplement the reason summary\n",
    "with engine.connect() as conn:\n",
    "    update_sql = text(\"\"\"\n",
    "        UPDATE esslnc\n",
    "        SET disease_related = 1, Organism = 'Human', reason_summary = '[ClinVar] Associated with variants in ClinVar.'\n",
    "        WHERE vitro = 0 AND vivo = 0 AND cancer_related = 0\n",
    "    \"\"\")\n",
    "    conn.execute(update_sql)\n",
    "    conn.commit()\n",
    "\n",
    "print(\"Update successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2.3 Import entries from dbesslnc1.0 that have been supplemented with annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.dbesslnc_gene.csv, additional annotation information was supplemented.\n",
    "input_file = '../dbesslnc/dbesslnc_gene.tsv'\n",
    "df = pd.read_csv(input_file, sep='\\t', header=0,\n",
    "                 names=['chr', 'start', 'end', 'strand', 'Lncbook_id', 'gene_name',  'NCBI_id', 'Noncode_id'])\n",
    "df['start'] = df['start']+1;\n",
    "print(len(df))\n",
    "with engine.connect() as conn:\n",
    "    for _, row in df.iterrows():\n",
    "        try:\n",
    "            insert_sql = text(\"\"\"\n",
    "                INSERT IGNORE INTO esslnc \n",
    "                (chr, start, end, Lncbook_id, Noncode_id, strand, gene_name, NCBI_id)\n",
    "                VALUES (:chr, :start, :end, :Lncbook_id, :Noncode_id, :strand, :gene_name, :NCBI_id)\n",
    "            \"\"\")\n",
    "            conn.execute(insert_sql, row.to_dict())\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f\"skip\")\n",
    "            continue\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.unmap_from_dbesslnc.txt,After manually searching the public database GeneCards, \n",
    "# additional annotation information was supplemented.\n",
    "input_file = '../dbesslnc/unmap_from_dbesslnc.tsv'\n",
    "df = pd.read_csv(input_file, sep='\\t')\n",
    "print(len(df))\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    for _, row in df.iterrows():\n",
    "        try:\n",
    "            insert_sql = text(\"\"\"\n",
    "                INSERT IGNORE INTO esslnc \n",
    "                (chr, start, end, Lncbook_id, Noncode_id, strand, gene_name, NCBI_id,PMID)\n",
    "                VALUES (:chr, :start, :end, :Lncbook_id, :Noncode_id, :strand, :gene_name, :NCBI_id, :PMID)\n",
    "            \"\"\")\n",
    "            conn.execute(insert_sql, row.to_dict())\n",
    "        except Exception as e:\n",
    "            print(f\"error {str(e)}\")\n",
    "            print(f\"skip\")\n",
    "            continue\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. For lncRNAs from dbesslnc,mark the columns for cancer-related and in vivo.\n",
    "# dbesslnc_id.txt,The file contains the gene name and the corresponding role and lit.\n",
    "df = pd.read_csv('../dbesslnc/dbesslnc_reason.tsv', sep='\\t')\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    general_genes = tuple(df[df['Role'] == 'General']['Name'].tolist())\n",
    "    suppressor_genes = tuple(df[df['Role'] == 'Tumor suppressor gene']['Name'].tolist())\n",
    "    oncogenes = tuple(df[df['Role'] == 'Oncogene']['Name'].tolist())\n",
    "    for _,row in df.iterrows():\n",
    "        if row['Name'] in general_genes:\n",
    "            update_general = text(\"\"\"\n",
    "                UPDATE esslnc \n",
    "                SET vivo = 1, reason_summary = :Reason, PMID = :PMID, Organism = 'Human'\n",
    "                WHERE gene_name = :genes and (vitro = 0 AND disease_related = 0)\n",
    "            \"\"\")\n",
    "            conn.execute(update_general, {'genes': row['Name'], 'Reason': row['Reason'], 'PMID': row['PMID']})\n",
    "\n",
    "        if row['Name'] in suppressor_genes:\n",
    "            update_suppressor = text(\"\"\"\n",
    "                UPDATE esslnc \n",
    "                SET cancer_related = 2, reason_summary =  :Reason, PMID = :PMID, Organism = 'Human'\n",
    "                WHERE gene_name = :genes and (vitro = 0 AND disease_related = 0)\n",
    "            \"\"\")\n",
    "            conn.execute(update_suppressor, {'genes': row['Name'], 'Reason': row['Reason'], 'PMID': row['PMID']})\n",
    "\n",
    "        if row['Name'] in oncogenes:\n",
    "            update_oncogene = text(\"\"\"\n",
    "                UPDATE esslnc \n",
    "                SET cancer_related = 1, reason_summary =  :Reason, PMID = :PMID, Organism = 'Human'\n",
    "                WHERE gene_name = :genes and (vitro = 0 AND disease_related = 0)\n",
    "            \"\"\")\n",
    "            conn.execute(update_oncogene, {'genes': row['Name'], 'Reason': row['Reason'], 'PMID': row['PMID']})\n",
    "    \n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supplement genesummary information\n",
    "#\n",
    "import re\n",
    "def process_ncbi_mappings_and_summaries(summary_file):\n",
    "    \"\"\"Read NCBI mapping files, extract gene IDs, and match gene summary information\"\"\"\n",
    "    \n",
    "    # Collect gene IDs\n",
    "    all_gene_ids = set()\n",
    "    read_sql = text(\"\"\"\n",
    "        SELECT DISTINCT NCBI_id\n",
    "        FROM esslnc\n",
    "        WHERE NCBI_id IS NOT NULL AND NCBI_id != 'N.A.'\n",
    "    \"\"\")\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(read_sql)\n",
    "        all_gene_ids.update(row[0] for row in result if row[0] is not None)\n",
    "\n",
    "    # Read gene summary file\n",
    "    gene_summary_df = pd.read_csv(summary_file, sep='\\t')\n",
    "    gene_summary_df['GeneID'] = gene_summary_df['GeneID'].astype(str)\n",
    "    \n",
    "    gene_summary_df = gene_summary_df[gene_summary_df['GeneID'].isin(all_gene_ids)]\n",
    "    \n",
    "    # print(all_gene_ids)\n",
    "    # Clean Summary column\n",
    "    def clean_summary(summary):\n",
    "        if pd.isna(summary):\n",
    "            return ''\n",
    "        return re.sub(r'\\s*\\[provided by.*$', '', str(summary), flags=re.IGNORECASE).strip()\n",
    "    \n",
    "    gene_summary_df['Summary_cleaned'] = gene_summary_df['Summary'].apply(clean_summary)\n",
    "    print(gene_summary_df.head(10))\n",
    "\n",
    "    # Match gene IDs and summaries\n",
    "    results = []\n",
    "    for gene_id in all_gene_ids:\n",
    "        match = gene_summary_df[gene_summary_df['GeneID'] == gene_id]\n",
    "        if not match.empty:\n",
    "            summary = match.iloc[0]['Summary_cleaned']\n",
    "            results.append({'GeneID': gene_id, 'Summary': summary})\n",
    "    with engine.connect() as conn:\n",
    "        update_sql = text(\"\"\"\n",
    "            UPDATE esslnc\n",
    "            SET reason_summary = CONCAT(reason_summary, :summary)\n",
    "            WHERE NCBI_id = :gene_id AND cancer_related = 0 AND vivo = 0\n",
    "            \"\"\")\n",
    "        for result in results:\n",
    "            conn.execute(update_sql, {'summary': result['Summary'], 'gene_id': result['GeneID']})\n",
    "process_ncbi_mappings_and_summaries('../match/gene_summary')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2.4: Import the mouse's essential lncRNA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mouse essential lncRNA data\n",
    "df = pd.read_csv('../dbesslnc/dbesslnc_mouse.csv', sep=',', encoding='latin-1')\n",
    "\n",
    "for column in df.columns:\n",
    "    df[column] = df[column].astype(str).str.strip('\"')\n",
    "\n",
    "if 'reason_summary' in df.columns:\n",
    "    df['reason_summary'] = df['reason_summary'].apply(lambda x: f\"[Literature] {x}\" if x != 'N.A.' else x)\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    df.to_sql('esslnc', \n",
    "              con=conn, \n",
    "              if_exists='append',\n",
    "              index=False)\n",
    "    update_sql = text(\"\"\"\n",
    "        UPDATE esslnc\n",
    "        SET vivo = 1\n",
    "        WHERE Organism = 'Mouse'\n",
    "    \"\"\")\n",
    "    conn.execute(update_sql)\n",
    "    conn.commit()\n",
    "    \n",
    "print(f\"successfully {len(df)} rows to esslnc table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3: Generating Unique Identifiers for lncRNA Entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Generating Unique Identifiers for lncRNA Entries\n",
    "query = \"\"\"\n",
    "SELECT num_id, chr, start,end\n",
    "FROM esslnc \n",
    "WHERE Organism = 'Human'\n",
    "ORDER BY chr, start, end\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "df = pd.read_sql(query, engine)\n",
    "df['new_uid'] = ['ELH{:06d}'.format(i+1) for i in range(len(df))]\n",
    "\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    for index, row in df.iterrows():\n",
    "        update_sql = text(\"\"\"\n",
    "        UPDATE esslnc \n",
    "        SET UID_temp = :new_uid \n",
    "        WHERE num_id = :num_id\n",
    "        \"\"\")\n",
    "        conn.execute(update_sql, {\"new_uid\": row['new_uid'], \"num_id\": row['num_id']})\n",
    "    conn.commit()\n",
    "\n",
    "print(f\"Successfully updated {len(df)} records.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export Create a local mapping table for Temporary handling\n",
    "query1 = text(\"\"\"\n",
    "    SELECT UID,Lncbook_id,Noncode_id,chr,start,end,strand\n",
    "    FROM esslnc \n",
    "    WHERE disease_related = 1;\n",
    "\"\"\")\n",
    "query2 = text(\"\"\" \n",
    "    select UID,target,PMID from esslnc where vitro = 1;\n",
    "\"\"\")\n",
    "query3 = text(\"\"\"\n",
    "    SELECT UID,Lncbook_id,Noncode_id,chr,start,end,strand\n",
    "    FROM esslnc\n",
    "    WHERE (cancer_related > 0 or vitro = 1) and Organism != 'Mouse';\n",
    "\"\"\")\n",
    "output_file1 = 'variants_mapping.txt'\n",
    "output_file2 = 'crispr_mapping.txt'\n",
    "output_file3 = 'dbesslnc_mapping.txt'\n",
    "with engine.connect() as conn:\n",
    "\n",
    "    result1 = pd.read_sql(query1, conn)\n",
    "    result2 = pd.read_sql(query2, conn)\n",
    "    result3 = pd.read_sql(query3, conn)\n",
    "    result1.to_csv(output_file1, \n",
    "                   sep='\\t', \n",
    "                   index=False, \n",
    "                  header=True,\n",
    "                  na_rep='N.A.')\n",
    "\n",
    "    result2.to_csv(output_file2, \n",
    "                   sep='\\t', \n",
    "                   index=False, \n",
    "                   header=True,\n",
    "                   na_rep='N.A.')\n",
    "    \n",
    "    result3.to_csv(output_file3, \n",
    "                   sep='\\t', \n",
    "                   index=False, \n",
    "                   header=True,\n",
    "                   na_rep='N.A.')\n",
    "print(f\"exported {len(result1)} records {output_file1}\")\n",
    "print(f\"exported {len(result2)} records {output_file2}\")\n",
    "print(f\"exported {len(result3)} records {output_file3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export Create a local mapping table for Temporary handling\n",
    "query1 = text(\"\"\"\n",
    "    SELECT UID,Lncbook_id,Lncbook_trans_id,transcript_id\n",
    "    FROM trans\n",
    "    WHERE Organism != 'Mouse';\n",
    "\"\"\")\n",
    "\n",
    "output_file1 = 'UID_trans.txt'\n",
    "with engine.connect() as conn:\n",
    "\n",
    "    result1 = pd.read_sql(query1, conn)\n",
    "\n",
    "\n",
    "    result1.to_csv(output_file1, \n",
    "                   sep='\\t', \n",
    "                   index=False, \n",
    "                  header=True,\n",
    "                  na_rep='N.A.')\n",
    "print(f\"exported {len(result1)} records {output_file1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4: Import transcript Entries\n",
    "The required files:\n",
    "1. /match/seq_splice.bed seq_delete.bed seq_casrx.bed seq_crispri.bed\n",
    "2. /match/esslnc2.fa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import Genomic location information.\n",
    "import pandas as pd\n",
    "\n",
    "# custom bed file, generated by gen_fa.ipynb step1, Just input different files as needed\n",
    "custom_bed_file_path = '../match/test/temp_seq/'\n",
    "#transcript sequence crispr_gene.fa\n",
    "seq_file = '../match/test/output/lncRNA2.fa'\n",
    "def load_seq(seq_file,custom_bed_file):\n",
    "    # Load the mapping file\n",
    "    mapping_df = pd.read_csv('crispr_mapping.txt', sep='\\t',names=['UID','target'])\n",
    "    target_to_uid = {}\n",
    "\n",
    "    for _, row in mapping_df.iterrows():\n",
    "        target_to_uid[row['target']] = row['UID']\n",
    "\n",
    "    df = pd.read_csv(custom_bed_file, sep='\\t', header=None, \n",
    "                     names=['chr','start','end','name','score','strand','block_starts','block_sizes'])\n",
    "\n",
    "\n",
    "    def parse_fasta(fasta_file):\n",
    "        sequences = {}\n",
    "        current_seq_id = None\n",
    "        current_seq = []\n",
    "\n",
    "        with open(fasta_file) as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line.startswith('>'):\n",
    "\n",
    "                    if current_seq_id:\n",
    "                        sequences[current_seq_id] = ''.join(current_seq)\n",
    "                    current_seq_id = line[1:]  \n",
    "                    current_seq = []\n",
    "                else:\n",
    "                    current_seq.append(line)\n",
    "        if current_seq_id:\n",
    "            sequences[current_seq_id] = ''.join(current_seq)\n",
    "\n",
    "        return sequences\n",
    "\n",
    "\n",
    "    transcript_fa = parse_fasta(seq_file)\n",
    "\n",
    "    def process_transcript(row):\n",
    "\n",
    "        transcript_id = row['name'].rsplit('-',1)[1]\n",
    "        target = row['name'].rsplit('-',1)[0]\n",
    "        UID = target_to_uid.get(target)\n",
    "        fasta_seq = transcript_fa.get(transcript_id)\n",
    "        if fasta_seq is None:\n",
    "            FASTA = f\">{transcript_id}<br/>Sequence not found\"\n",
    "        else:\n",
    "            FASTA = f\">{transcript_id}<br/>{fasta_seq}\"\n",
    "\n",
    "        starts = [int(x) for x in row['block_starts'].split(',') if x]\n",
    "        sizes = [int(x) for x in row['block_sizes'].split(',') if x]\n",
    "\n",
    "\n",
    "        exon_positions = []\n",
    "        for rel_start, size in zip(starts, sizes):\n",
    "            abs_start = row['start'] + rel_start + 1  # 1-base\n",
    "            abs_end = abs_start + size - 1\n",
    "            exon_positions.append(f\"{abs_start}-{abs_end}\")\n",
    "\n",
    "        return {\n",
    "            'UID': UID,\n",
    "            'transcript_id': transcript_id,\n",
    "            'chr': row['chr'],\n",
    "            'start': row['start'] + 1,  # 1-base\n",
    "            'end': row['end'],\n",
    "            'length': sum(sizes),\n",
    "            'exon_num': len(sizes),\n",
    "            'exon_pos': ','.join(exon_positions),\n",
    "            'strand': row['strand'],\n",
    "            'FASTA':FASTA\n",
    "        }\n",
    "\n",
    "    result_df = pd.DataFrame([process_transcript(row) for _, row in df.iterrows()])\n",
    "\n",
    "\n",
    "    result_df.to_sql('trans', engine, if_exists='append', index=False)\n",
    "\n",
    "    print(f\"Successfully import {len(result_df)} records to trans table.\")\n",
    "\n",
    "load_seq(seq_file,custom_bed_file=custom_bed_file_path + 'seq_casrx38.bed')\n",
    "load_seq(seq_file,custom_bed_file=custom_bed_file_path + 'seq_delete38.bed')\n",
    "load_seq(seq_file,custom_bed_file=custom_bed_file_path + 'seq_i38.bed')\n",
    "load_seq(seq_file,custom_bed_file=custom_bed_file_path + 'seq_splice38.bed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transcript sequences from variants mapping file\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import text\n",
    "def sup_trans(trans_file):\n",
    "    # This function imports transcript sequences from a CSV file into the database\n",
    "    headers = pd.read_csv(trans_file, nrows=0).columns.tolist()\n",
    "    print(headers)\n",
    "    # chunk = chunk.fillna('N.A.')\n",
    "    # headers.remove('source')\n",
    "    sql = text(f\"\"\"\n",
    "        INSERT IGNORE INTO trans\n",
    "        ({', '.join(headers)})\n",
    "        VALUES ({', '.join([':' + col for col in headers])})\n",
    "    \"\"\")\n",
    "\n",
    "    total_rows = 0\n",
    "    success_rows = 0\n",
    "\n",
    "    for chunk in pd.read_csv(trans_file, chunksize=1000):\n",
    "        chunk = chunk.fillna('N.A.')\n",
    "        with engine.connect() as conn:\n",
    "            for _, row in chunk.iterrows():\n",
    "                try:\n",
    "                    result = conn.execute(sql, row.to_dict())\n",
    "                    success_rows += result.rowcount\n",
    "                    total_rows += 1\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    continue\n",
    "            conn.commit()\n",
    "    print(f\"Total processed: {total_rows}\")\n",
    "    print(f\"Successfully imported: {success_rows}\")\n",
    "    print(f\"Skipped duplicates: {total_rows - success_rows}\")\n",
    "# sup_trans(trans_file='../clinvar_map/db/exon/disease_trans.csv')\n",
    "sup_trans(trans_file='../clinvar_map/db/exon/dbesslnc_trans.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Import sequence (lncRNA assosiated with variants)\n",
    "The required files(downloaded from lncbook v2.0 and Noncode V6.0)\n",
    "1. LncBookv2_OnlyLnc.fa\n",
    "2. outLncRNA.fa \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seq from lncbook/noncode\n",
    "import pandas as pd\n",
    "from sqlalchemy import text\n",
    "def sup_fa(fa_file):\n",
    "        # This function imports sequences from a FASTA file into the database\n",
    "\n",
    "\n",
    "    sql_select = text(\"\"\"\n",
    "        SELECT transcript_id \n",
    "        FROM trans \n",
    "        WHERE FASTA IS NULL\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "    fasta_dict = {}\n",
    "    with open(fa_file, 'r') as f:\n",
    "        seq = ''\n",
    "        tid = ''\n",
    "        for line in f:\n",
    "            if line.startswith('>'):\n",
    "                if tid and seq:\n",
    "                    fasta_dict[tid] = seq\n",
    "                header = line.strip().lstrip('>')\n",
    "                tid = header.split()[0]  # Use the first part of the header as transcript_id\n",
    "                seq = ''\n",
    "            else:\n",
    "                seq += line.strip()\n",
    "        if tid and seq:\n",
    "            fasta_dict[tid] = seq\n",
    "\n",
    "\n",
    "    sql_update = text(\"\"\"\n",
    "        UPDATE trans \n",
    "        SET FASTA = :fasta , length = :length\n",
    "        WHERE transcript_id = :tid\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "    with engine.connect() as conn:\n",
    "        transcript_ids = conn.execute(sql_select).fetchall()\n",
    "        print(f\"Found {len(transcript_ids)} transcripts without FASTA data.\")\n",
    "        for (tid,) in transcript_ids:\n",
    "            print(f\"Processing transcript ID: {tid}\")\n",
    "            if tid in fasta_dict:\n",
    "                fasta_content = f\">{tid}<br/>{fasta_dict[tid]}\"\n",
    "                length = len(fasta_dict[tid])\n",
    "                try:\n",
    "                    print(f\"Updating {tid} with length {length}\")\n",
    "                    conn.execute(sql_update, {\"fasta\": fasta_content, \"tid\": tid, \"length\": length})\n",
    "                except Exception as e:\n",
    "                    print(f\"Error updating {tid}: {e}\")\n",
    "\n",
    "        conn.commit()\n",
    "sup_fa('LncBookv2_OnlyLnc.fa')\n",
    "sup_fa('outLncRNA.fa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Supplement Organism\n",
    "with engine.connect() as conn:\n",
    "    update_sql = text(\"\"\"\n",
    "        UPDATE trans\n",
    "        SET Organism = 'Human'\n",
    "        WHERE Organism IS NULL;\n",
    "    \"\"\")\n",
    "    conn.execute(update_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import Mouse transcripts from dbesslnc\n",
    "The required files:\n",
    "1. /dbesslnc/mouse_trans.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mouse essential lncRNA data\n",
    "df = pd.read_csv('../dbesslnc/mouse_trans.csv', sep=',', usecols=lambda column: column != 'Name')\n",
    "df = df.rename(columns={'chrom': 'chr'})\n",
    "for column in df.columns:\n",
    "    df[column] = df[column].astype(str).str.strip('\"')\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    df.to_sql('trans', \n",
    "              con=conn, \n",
    "              if_exists='append',\n",
    "              index=False)\n",
    "print(f\"successfully {len(df)} rows to trans table.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5: Import the mapped transcript IDs\n",
    "The required files\n",
    "1. /match/lncbook_map.tsv\n",
    "2. /match/noncode_map.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the mapped database gene IDs and transcript IDs,along with other mapped data.\n",
    "# You can modify SQL statements and input files to update fields in the database.\n",
    "\n",
    "def sup_linkID(map_file,db):\n",
    "    # This function imports sequences from a FASTA file into the database\n",
    "\n",
    "    map_df = pd.read_csv(map_file, sep='\\t', header=None)\n",
    "    # filter\n",
    "    map_dict = map_df[map_df[5] == 'transcript'][[1,2,3]].drop_duplicates()\n",
    "    lb = ['transcript_id','Lncbook_trans_id', 'Lncbook_id']\n",
    "    nc = ['transcript_id','Noncode_trans_id', 'Noncode_id']\n",
    "    \n",
    "\n",
    "    for _, lnc_row in map_dict.iterrows():\n",
    "        try:\n",
    "            with engine.connect() as conn:\n",
    "                update_sql = text(f\"\"\"\n",
    "                UPDATE trans \n",
    "                SET {lb[1] if db == 'lncbook' else nc[1]} = :db_trans_id, \n",
    "                    {lb[2] if db == 'lncbook' else nc[2]} = :db_id\n",
    "                WHERE transcript_id = :transcript_id\n",
    "                \"\"\")\n",
    "\n",
    "                result = conn.execute(update_sql, {\n",
    "                    \"transcript_id\": lnc_row[1],\n",
    "                    \"db_trans_id\": lnc_row[2],\n",
    "                    \"db_id\": lnc_row[3]\n",
    "                })\n",
    "\n",
    "                rows_affected = result.rowcount\n",
    "                # print(f\"update records: {lnc_row['transcript_id']}, affected rows: {rows_affected}\")\n",
    "\n",
    "                conn.commit()\n",
    "        except Exception as e:\n",
    "            print(f\"update fail: {lnc_row['transcript_id']}, error: {str(e)}\")\n",
    "\n",
    "\n",
    "sup_linkID(map_file = '../match/test/map/lncbook_map_trans.tsv',db = 'lncbook' )\n",
    "sup_linkID(map_file = '../match/test/map/noncode_map_trans.tsv',db = 'noncode' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6: import CRISPR experiment record \n",
    "`/curated/exp_crispr.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "exp_df = pd.read_csv('../curated/exp_crispr.csv')\n",
    "\n",
    "exp_df.to_sql('exp_crispr', \n",
    "              engine, \n",
    "              if_exists='append', \n",
    "              index=False,         \n",
    "              chunksize=1000)      \n",
    "\n",
    "print(f\"Successfully import {len(exp_df)} records to trans table.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step6.1 \n",
    "Group by   exp_type   and   target_id   columns, for groups with a count of 1, mark as 'cell-line specific' and update in the table; for groups with a count of 2-5, mark as 'common essential'; and for groups with a count greater than 5, mark as 'core essential'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import text\n",
    "\n",
    "\n",
    "sql_select = text(\"\"\"\n",
    "    SELECT exp_type, target_id \n",
    "    FROM exp_crispr\n",
    "\"\"\")\n",
    "\n",
    "with engine.connect() as conn:\n",
    "\n",
    "    df = pd.read_sql(sql_select, conn)\n",
    "    \n",
    "\n",
    "    group_counts = df.groupby(['exp_type', 'target_id']).size()\n",
    "    \n",
    "\n",
    "    cell_specific = group_counts[group_counts == 1].index\n",
    "    common = group_counts[(group_counts >= 2) & (group_counts <= 5)].index\n",
    "    core = group_counts[group_counts > 5].index\n",
    "    \n",
    "\n",
    "    update_sql = text(\"\"\"\n",
    "        UPDATE exp_crispr\n",
    "        SET role = :etype \n",
    "        WHERE exp_type = :exp AND target_id = :tid\n",
    "    \"\"\")\n",
    "    \n",
    "\n",
    "    for exp, tid in cell_specific:\n",
    "        conn.execute(update_sql, {\"etype\": \"cell-line specific\", \"exp\": exp, \"tid\": tid})\n",
    "        \n",
    "    for exp, tid in common:\n",
    "        conn.execute(update_sql, {\"etype\": \"common essential\", \"exp\": exp, \"tid\": tid})\n",
    "        \n",
    "    for exp, tid in core:\n",
    "        conn.execute(update_sql, {\"etype\": \"core essential\", \"exp\": exp, \"tid\": tid})\n",
    "    \n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step6.2: Assign generated UIDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "map_file = 'crispr_mapping.txt' #*_map.tsv,\n",
    "\n",
    "map_df = pd.read_csv(map_file, sep='\\t', names=['UID','target','PMID'])\n",
    "# filter\n",
    "\n",
    "for _, lnc_row in map_df.iterrows():\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            update_sql = text(\"\"\"\n",
    "            UPDATE exp_crispr \n",
    "            SET UID= :UID\n",
    "            WHERE target_id = :target AND PMID = :PMID\n",
    "            \"\"\")\n",
    "            \n",
    "            result = conn.execute(update_sql, {\n",
    "                \"UID\": lnc_row['UID'],\n",
    "                \"target\": lnc_row['target'],\n",
    "                \"PMID\": lnc_row['PMID']\n",
    "            })\n",
    "            \n",
    "            rows_affected = result.rowcount            \n",
    "            conn.commit()\n",
    "    except Exception as e:\n",
    "        print(f\"update fail: {lnc_row['target']}, error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step7ï¼šimport variants Mapping table and variants\n",
    "The required files.\n",
    "1. /clinvar_map/db/construct_map/final_mapping.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import variant information and mapping tables into the database.\n",
    "import pandas as pd\n",
    "from sqlalchemy import text\n",
    "\n",
    "variants_file = '../clinvar_map/db/construct_map/final_mapping.csv'\n",
    "headers = pd.read_csv(variants_file, nrows=0).columns.tolist()\n",
    "\n",
    "#table name variants/lncrna_variant_mapping\n",
    "sql = text(f\"\"\"\n",
    "    INSERT IGNORE INTO lncrna_variant_mapping\n",
    "    ({', '.join(headers)})\n",
    "    VALUES ({', '.join([':' + col for col in headers])})\n",
    "\"\"\")\n",
    "\n",
    "total_rows = 0\n",
    "success_rows = 0\n",
    "\n",
    "for chunk in pd.read_csv(variants_file, chunksize=1000):\n",
    "    with engine.connect() as conn:\n",
    "        for _, row in chunk.iterrows():\n",
    "            try:\n",
    "                result = conn.execute(sql, row.to_dict())\n",
    "                success_rows += result.rowcount\n",
    "                total_rows += 1\n",
    "            except Exception as e:\n",
    "                continue\n",
    "        conn.commit()\n",
    "print(f\"Total processed: {total_rows}\")\n",
    "print(f\"Successfully imported: {success_rows}\")\n",
    "print(f\"Skipped duplicates: {total_rows - success_rows}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step8: import expression profile\n",
    "The required file\n",
    "1. /clinvar_map/db/exp/exp_profile.csv\n",
    "2. /dbesslnc/expression_mouse.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all lncRNA expression profile\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import text\n",
    "def import_expression_profile(expression_file,table_name):\n",
    " \n",
    "    headers = pd.read_csv(expression_file, nrows=0).columns.tolist()\n",
    "    print(headers)\n",
    "\n",
    "    sql = text(f\"\"\"\n",
    "        INSERT IGNORE INTO {table_name}\n",
    "        ({', '.join(headers)})\n",
    "        VALUES ({', '.join([':' + col for col in headers])})\n",
    "    \"\"\")\n",
    "    update_sql = text(f\"\"\"\n",
    "                      UPDATE {table_name}\n",
    "                      SET Organism = 'Human'\n",
    "    \"\"\")\n",
    "\n",
    "    total_rows = 0\n",
    "    success_rows = 0\n",
    "    if table_name == 'exp_profile':\n",
    "        with engine.connect() as conn:\n",
    "            conn.execute(update_sql)\n",
    "\n",
    "    for chunk in pd.read_csv(expression_file, chunksize=1000):\n",
    "        with engine.connect() as conn:\n",
    "            for _, row in chunk.iterrows():\n",
    "                try:\n",
    "                    result = conn.execute(sql, row.to_dict())\n",
    "                    success_rows += result.rowcount\n",
    "                    total_rows += 1\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    continue\n",
    "            conn.commit()\n",
    "    print(f\"Total processed: {total_rows}\")\n",
    "    print(f\"Successfully imported: {success_rows}\")\n",
    "    print(f\"Skipped duplicates: {total_rows - success_rows}\")\n",
    "import_expression_profile(expression_file='../clinvar_map/db/exp/exp_profile.csv', table_name='exp_profile')\n",
    "import_expression_profile(expression_file='../dbesslnc/expression_mouse.csv', table_name='expression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step9: All NULL entries in the database should be changed to N.A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as conn:\n",
    "    update_esslnc = text(\"\"\"\n",
    "    UPDATE esslnc \n",
    "    SET \n",
    "        NCBI_id = COALESCE(NCBI_id, 'N.A.'),\n",
    "        gene_name = COALESCE(gene_name, 'N.A.'),\n",
    "        Alias = COALESCE(Alias, 'N.A.'),\n",
    "        ensembl_id = COALESCE(ensembl_id, 'N.A.'),\n",
    "        Noncode_id = COALESCE(Noncode_id, 'N.A.'),\n",
    "        Lncbook_id = COALESCE(Lncbook_id, 'N.A.'),\n",
    "        target = COALESCE(target, 'N.A.')\n",
    "    \"\"\")\n",
    "    conn.execute(update_esslnc)\n",
    "  \n",
    "\n",
    "    update_trans = text(\"\"\"\n",
    "    UPDATE trans\n",
    "    SET \n",
    "        Noncode_id = COALESCE(Noncode_id, 'N.A.'),\n",
    "        Noncode_trans_id = COALESCE(Noncode_trans_id, 'N.A.'),\n",
    "        Lncbook_id = COALESCE(Lncbook_id, 'N.A.'),\n",
    "        Lncbook_trans_id = COALESCE(Lncbook_trans_id, 'N.A.')\n",
    "    \"\"\")\n",
    "    conn.execute(update_trans)\n",
    "    conn.commit()\n",
    "\n",
    "print(\"NULL2N.A. Update successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step10. Generate the FA file used by the BLAST service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the FA file used by the BLAST service\n",
    "import pandas as pd\n",
    "from sqlalchemy import text\n",
    "query = text(\"\"\"\n",
    "    SELECT transcript_id, FASTA \n",
    "    FROM trans \n",
    "    WHERE FASTA IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "output_file = 'lncRNA2.fasta'\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(query)\n",
    "    rows = result.fetchall()\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        for transcript_id, fasta_content in rows:\n",
    "            if fasta_content and '<br/>' in fasta_content:\n",
    "          \n",
    "                sequence = fasta_content.split('<br/>', 1)[1]\n",
    "                \n",
    "                f.write(f\">{transcript_id}\\n\")\n",
    "                f.write(f\"{sequence}\\n\")\n",
    "            else:\n",
    "                print(f\"Warning: Invalid FASTA format for {transcript_id}\")\n",
    "\n",
    "print(f\"Successfully generated {output_file} with {len(rows)} sequences\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
