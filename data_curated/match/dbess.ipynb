{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create a database connection and Import data\n",
    "\n",
    "\n",
    "**Preparations before executing the code**\n",
    "```\n",
    "## Create a database\n",
    "create database ess_test;\n",
    "use ess_test;\n",
    "\n",
    "## Import table structure\n",
    "source /your_path/store/dbess_schema.sql;\n",
    "\n",
    "## modify the database table structure\n",
    "ALTER TABLE esslnc\n",
    "ADD CONSTRAINT unique_lnc \n",
    "UNIQUE (chr, start, end, strand);\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine,text\n",
    "\n",
    "\n",
    "username = 'root'\n",
    "password = 'root'\n",
    "host = 'localhost'  # general 'localhost'\n",
    "port = 3307  #Default MySQL port number.\n",
    "database = 'ess_test'\n",
    "\n",
    "\n",
    "engine = create_engine(f'mysql+pymysql://{username}:{password}@{host}:{port}/{database}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2: Import gene entries\n",
    "\n",
    "The files needed.\n",
    "1. /match/test/map/lncbook_map.tsv, res_lncbook_map.tsv\n",
    "2. /match/test/map/noncode_map.tsv, res_noncode_map.tsv\n",
    "3. /match/test/map/gencode_map.tsv, res_gencode_map.tsv\n",
    "4. /match/test/map/ncbi_map.tsv, res_ncbi_map.tsv\n",
    "5. /match/go_map.txt\n",
    "6. /match/crispr_all.bed\n",
    "7. /match/merge.txt\n",
    "8. /clinvar_map/db/crispr_overlap/final_lncRNA_nocrispr.bed\n",
    "9. /cancer/unmap_from_dbesslnc.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2.1 : import lncRNA verified by CRISPR \n",
    " 1. main crispr lncRNA gene\n",
    " 2. update vitro column\n",
    " 3. Export data for variants mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_path = '../match/'\n",
    "crispr_bed_file = relative_path + 'test/output/crispr_all.bed'\n",
    "\n",
    "df = pd.read_csv(crispr_bed_file, sep='\\t', header=None)\n",
    "\n",
    "df['gene_id'] = df[3].apply(lambda x: x.rsplit('-', 1)[0])\n",
    "\n",
    "# Directly use the gene_id as the grouping basis without merging.\n",
    "df['merge_group'] = df['gene_id']\n",
    "\n",
    "# group by gene_id and merge records\n",
    "result = df.groupby('merge_group').agg({\n",
    "    0: 'first',  # chr\n",
    "    1: lambda x: min(x) + 1,    # start\n",
    "    2: 'max',    # end\n",
    "    5: 'first',  # strand\n",
    "    'gene_id': lambda x: ';'.join(sorted(set(x)))  \n",
    "}).reset_index()\n",
    "\n",
    "result = result[[0, 1, 2, 5, 'gene_id']]  \n",
    "result.columns = ['chr', 'start', 'end', 'strand', 'target']  \n",
    "\n",
    "result.to_sql('esslnc', engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert in the manually retrieved supplementary lncRNA gene entries\n",
    "data = [\n",
    "        {'Noncode_id': 'N.A.', 'Lncbook_id': 'N.A.', 'ensembl_id': 'N.A.', \n",
    "         'target': 'LH00477', 'chr': 'chr1', 'start': 145410838, \n",
    "         'end': 145413269, 'strand': '+'},\n",
    "        {'Noncode_id': 'NONHSAG005780.3', 'Lncbook_id': 'HSALNG0077928', \n",
    "         'ensembl_id': 'ENSG000000290921.2', 'target': 'LH02126', \n",
    "         'chr': 'chr10', 'start': 45888164, 'end': 45972422, 'strand': '+'},\n",
    "        {'Noncode_id': 'NONHSAG097226.1', 'Lncbook_id': 'HSALNG0060060', \n",
    "         'ensembl_id': 'N.A.', 'target': 'LH14878', 'chr': 'chr7', \n",
    "         'start': 100963828, 'end': 100968124, 'strand': '-'}\n",
    "    ]\n",
    "    \n",
    "    # 2. Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.to_sql('esslnc', engine, if_exists='append', index=False)\n",
    "print(f\"Successfully inserted {len(df)} supplementary lncRNA gene entries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. update vitro and Organism column\n",
    "with engine.connect() as conn:\n",
    "    update_sql = text(\"\"\"\n",
    "        UPDATE esslnc \n",
    "        SET vitro = 1,Organism = 'Human'\n",
    "    \"\"\")\n",
    "    conn.execute(update_sql)\n",
    "    conn.commit()\n",
    "\n",
    "print(\"Update successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the externally mapped database ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the mapped database gene IDs and gene names,along with other mapped data(lncbook/ncbi/noncode/gencode_map.tsv)\n",
    "\n",
    "# for lncbook and noncode IDs\n",
    "def import_mapped_data(map_file,db):\n",
    "    # for lncbook and noncode IDs\n",
    "    map_df = pd.read_csv(map_file, sep='\\t', header=None)\n",
    "    map_dict = map_df[[0,3,4]].drop_duplicates()\n",
    "    with engine.connect() as conn:\n",
    "        for _, row in map_dict.iterrows():\n",
    "            if db != 'Noncode_id':\n",
    "                update_sql_name = text(f\"\"\"\n",
    "                    UPDATE esslnc \n",
    "                    SET gene_name = :gene_name\n",
    "                    WHERE target LIKE :pattern1 \n",
    "                    OR target LIKE :pattern2\n",
    "                    AND (gene_name IS NULL or gene_name = 'N.A.')\n",
    "                \"\"\")\n",
    "\n",
    "            update_sql = text(f\"\"\"\n",
    "                UPDATE esslnc \n",
    "                SET {db} = :id\n",
    "                WHERE target LIKE :pattern1 \n",
    "                OR target LIKE :pattern2\n",
    "            \"\"\")\n",
    "            pattern1 = f\"{row[0]}%\"  \n",
    "            pattern2 = f\"%;{row[0]}%\"  \n",
    "            gene_name = 'N.A.' if str(row[4]).startswith('ENSG') or str(row[4]).startswith('LOC') else row[4]\n",
    "            conn.execute(update_sql, {\n",
    "                \"id\": row[3], \n",
    "                \"pattern1\": pattern1,\n",
    "                \"pattern2\": pattern2\n",
    "            })\n",
    "            if db != 'Noncode_id':\n",
    "                conn.execute(update_sql_name, {\n",
    "                    \"gene_name\": gene_name,\n",
    "                    \"pattern1\": pattern1,\n",
    "                    \"pattern2\": pattern2\n",
    "                })\n",
    "            conn.commit()\n",
    "\n",
    "import_mapped_data('../match/test/map/ncbi_map.tsv', 'NCBI_id')\n",
    "import_mapped_data('../match/test/map/gencode_map.tsv', 'ensembl_id')\n",
    "import_mapped_data('../match/test/map/lncbook_map.tsv', 'Lncbook_id')\n",
    "import_mapped_data('../match/test/map/noncode_map.tsv', 'Noncode_id')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the reason summary of genes verified by CRISPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read exp_crispr.csv file\n",
    "df = pd.read_csv('../curated/exp_crispr.csv')\n",
    "\n",
    "# Group by target_id and collect all exp_type\n",
    "target_summary = df.groupby('target_id').agg({\n",
    "    'exp_type': lambda x: ', '.join(sorted(set(x))),\n",
    "    'PMID': lambda x: ','.join(set(str(p) for p in x if pd.notna(p)))\n",
    "}).to_dict('index')\n",
    "\n",
    "# Update database\n",
    "with engine.connect() as conn:\n",
    "    for target_id, data in target_summary.items():\n",
    "        exp_types = data['exp_type']\n",
    "        pmids = data['PMID']\n",
    "        reason_summary = f\"[{exp_types}] Verified by the {exp_types} experiment. \"\n",
    "        \n",
    "        update_sql = text(\"\"\"\n",
    "            UPDATE esslnc \n",
    "            SET reason_summary = :reason_summary,\n",
    "            PMID = :pmids\n",
    "            WHERE target = :target_id\n",
    "        \"\"\")\n",
    "        \n",
    "        conn.execute(update_sql, {\n",
    "            \"reason_summary\": reason_summary,\n",
    "            \"pmids\": pmids,\n",
    "            \"target_id\": target_id\n",
    "        })\n",
    "    \n",
    "    conn.commit()\n",
    "\n",
    "print(f\"Successfully updated {len(target_summary)} genes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2.3 import variants lncRNA gene to database,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.\n",
    "# Adjust the order of column names according to different files.\n",
    "\n",
    "input_file = '../clinvar_map/final_lncRNA_nocrispr.bed'\n",
    "df = pd.read_csv(input_file, sep='\\t', header=None,\n",
    "                 names=['chr', 'start', 'end', 'Lncbook_id', 'Noncode_id', \n",
    "                       'strand', 'gene_name', 'NCBI_id', 'variants_num','disease_related'])\n",
    "print(len(df))\n",
    "\n",
    "df['Lncbook_id'] = df['Lncbook_id'].replace('-', 'N.A.')\n",
    "df['Noncode_id'] = df['Noncode_id'].replace('-', 'N.A.')\n",
    "df['NCBI_id'] = df['NCBI_id'].replace('-', 'N.A.')\n",
    "df['gene_name'] = df['gene_name'].replace('-', 'N.A.')\n",
    "insert_data = df[['chr', 'start', 'end', 'Lncbook_id', 'Noncode_id', \n",
    "                  'strand', 'gene_name', 'NCBI_id','disease_related']]\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    for _, row in insert_data.iterrows():\n",
    "        try:\n",
    "            insert_sql = text(\"\"\"\n",
    "                INSERT IGNORE INTO esslnc \n",
    "                (chr, start, end, Lncbook_id, Noncode_id, strand, gene_name, NCBI_id,disease_related)\n",
    "                VALUES (:chr, :start, :end, :Lncbook_id, :Noncode_id, :strand, :gene_name, :NCBI_id,:disease_related)\n",
    "            \"\"\")\n",
    "            conn.execute(insert_sql, row.to_dict())\n",
    "        except Exception as e:\n",
    "            print(f\"skip\")\n",
    "            continue\n",
    "    conn.commit()\n",
    "\n",
    "\n",
    "# try:\n",
    "#     insert_data.to_sql('esslnc', engine, if_exists='append', index=False)\n",
    "# except Exception as e:\n",
    "#     print(f\"error {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Update disease_related column,Mark whether lncRNAs derived from CRISPR experiments are disease-related.\n",
    "# Supplement the reason summary\n",
    "with engine.connect() as conn:\n",
    "    update_sql = text(\"\"\"\n",
    "        UPDATE esslnc\n",
    "        SET disease_related = 1, reason_summary = '[ClinVar] Associated with variants in ClinVar.'\n",
    "        WHERE vitro = 0 AND vivo = 0 AND cancer_related = 0\n",
    "    \"\"\")\n",
    "    conn.execute(update_sql)\n",
    "    conn.commit()\n",
    "\n",
    "print(\"Update successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2.4 Import entries from dbesslnc that have been supplemented with annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.dbesslnc_gene.csv, additional annotation information was supplemented.\n",
    "input_file = 'dbesslnc_gene.csv'\n",
    "df = pd.read_csv(input_file, sep=',')\n",
    "df['start'] = df['start']+1;\n",
    "print(df.columns)\n",
    "with engine.connect() as conn:\n",
    "    for _, row in df.iterrows():\n",
    "        try:\n",
    "            insert_sql = text(\"\"\"\n",
    "                INSERT IGNORE INTO esslnc \n",
    "                (chr, start, end, Lncbook_id, Noncode_id, strand, gene_name, NCBI_id)\n",
    "                VALUES (:chr, :start, :end, :Lncbook_id, :Noncode_id, :strand, :gene_name, :NCBI_id)\n",
    "            \"\"\")\n",
    "            conn.execute(insert_sql, row.to_dict())\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f\"skip\")\n",
    "            continue\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.unmap_from_dbesslnc.txt,After manually searching the public database GeneCards, \n",
    "# additional annotation information was supplemented.\n",
    "input_file = 'unmap_from_dbesslnc.txt'\n",
    "df = pd.read_csv(input_file, sep='\\t')\n",
    "with engine.connect() as conn:\n",
    "    for _, row in df.iterrows():\n",
    "        try:\n",
    "            insert_sql = text(\"\"\"\n",
    "                INSERT IGNORE INTO esslnc \n",
    "                (chr, start, end, Lncbook_id, Noncode_id, strand, gene_name, NCBI_id,PMID)\n",
    "                VALUES (:chr, :start, :end, :Lncbook_id, :Noncode_id, :strand, :gene_name, :NCBI_id, :PMID)\n",
    "            \"\"\")\n",
    "            conn.execute(insert_sql, row.to_dict())\n",
    "        except Exception as e:\n",
    "            print(f\"skip\")\n",
    "            continue\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. For lncRNAs from dbesslnc,mark the columns for cancer-related and in vivo.\n",
    "# dbesslnc_id.txt,The file contains the gene name and the corresponding role and lit.\n",
    "df = pd.read_csv('dbesslnc_reason.txt', sep='\\t')\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    general_genes = tuple(df[df['Role'] == 'General']['Name'].tolist())\n",
    "    suppressor_genes = tuple(df[df['Role'] == 'Tumor suppressor gene']['Name'].tolist())\n",
    "    oncogenes = tuple(df[df['Role'] == 'Oncogene']['Name'].tolist())\n",
    "    for _,row in df.iterrows():\n",
    "        if row['Name'] in general_genes:\n",
    "            update_general = text(\"\"\"\n",
    "                UPDATE esslnc \n",
    "                SET vivo = 1, reason_summary = :Reason, PMID = :PMID\n",
    "                WHERE gene_name = :genes\n",
    "            \"\"\")\n",
    "            conn.execute(update_general, {'genes': row['Name'], 'Reason': row['Reason'], 'PMID': row['PMID']})\n",
    "\n",
    "        if row['Name'] in suppressor_genes:\n",
    "            update_suppressor = text(\"\"\"\n",
    "                UPDATE esslnc \n",
    "                SET cancer_related = 2, reason_summary =  :Reason, PMID = :PMID\n",
    "                WHERE gene_name = :genes\n",
    "            \"\"\")\n",
    "            conn.execute(update_suppressor, {'genes': row['Name'], 'Reason': row['Reason'], 'PMID': row['PMID']})\n",
    "\n",
    "        if row['Name'] in oncogenes:\n",
    "            update_oncogene = text(\"\"\"\n",
    "                UPDATE esslnc \n",
    "                SET cancer_related = 1, reason_summary =  :Reason, PMID = :PMID\n",
    "                WHERE gene_name = :genes\n",
    "            \"\"\")\n",
    "            conn.execute(update_oncogene, {'genes': row['Name'], 'Reason': row['Reason'], 'PMID': row['PMID']})\n",
    "    \n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2.5: Import the mouse's essential lncRNA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mouse essential lncRNA data\n",
    "df = pd.read_csv('../dbesslnc/dbesslnc_mouse.csv', sep=',', encoding='latin-1')\n",
    "\n",
    "for column in df.columns:\n",
    "    df[column] = df[column].astype(str).str.strip('\"')\n",
    "\n",
    "if 'reason_summary' in df.columns:\n",
    "    df['reason_summary'] = df['reason_summary'].apply(lambda x: f\"[Literature] {x}\" if x != 'N.A.' else x)\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    df.to_sql('esslnc', \n",
    "              con=conn, \n",
    "              if_exists='append',\n",
    "              index=False)\n",
    "    update_sql = text(\"\"\"\n",
    "        UPDATE esslnc\n",
    "        SET vivo = 1\n",
    "        WHERE Organism = 'Mouse'\n",
    "    \"\"\")\n",
    "    conn.execute(update_sql)\n",
    "    conn.commit()\n",
    "    \n",
    "print(f\"successfully {len(df)} rows to esslnc table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3: Generating Unique Identifiers for lncRNA Entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Generating Unique Identifiers for lncRNA Entries\n",
    "query = \"\"\"\n",
    "SELECT num_id, chr, start \n",
    "FROM esslnc \n",
    "WHERE Organism = 'Human'\n",
    "ORDER BY chr, start\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "df = pd.read_sql(query, engine)\n",
    "df['new_uid'] = ['ELH{:06d}'.format(i+1) for i in range(len(df))]\n",
    "\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    for index, row in df.iterrows():\n",
    "        update_sql = text(\"\"\"\n",
    "        UPDATE esslnc \n",
    "        SET UID = :new_uid \n",
    "        WHERE num_id = :num_id\n",
    "        \"\"\")\n",
    "        conn.execute(update_sql, {\"new_uid\": row['new_uid'], \"num_id\": row['num_id']})\n",
    "    conn.commit()\n",
    "\n",
    "print(f\"Successfully updated {len(df)} records。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step4.2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a local mapping table for crispr lncRNA transcript.\n",
    "query = \"\"\"\n",
    "SELECT UID, target \n",
    "FROM esslnc WHERE vitro = 1\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "\n",
    "def split_target(row):\n",
    "    targets = row['target'].split(';')\n",
    "    return [{'UID': row['UID'], 'target': target} for target in targets]\n",
    "\n",
    "\n",
    "expanded_rows = [item for _, row in df.iterrows() for item in split_target(row)]\n",
    "result_df = pd.DataFrame(expanded_rows)\n",
    "\n",
    "\n",
    "output_file = 'crispr_target_UID.tsv'\n",
    "result_df.to_csv(output_file, sep='\\t', index=False)\n",
    "\n",
    "print(f\"exported {len(result_df)} records {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step4.3: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a local mapping table for variants lncRNA \n",
    "query = \"\"\"\n",
    "SELECT UID, target \n",
    "FROM esslnc WHERE vitro = 1\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "output_file = 'crispr_UID.txt'\n",
    "df.to_csv(output_file, sep='\\t', index=False)\n",
    "\n",
    "print(f\"exported {len(df)} records {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step4.4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export Create a local mapping table for disease_related lncRNA.\n",
    "query = text(\"\"\"\n",
    "    SELECT Lncbook_id,Noncode_id,UID\n",
    "    FROM esslnc \n",
    "    WHERE disease_related = 1 AND vitro = 0;\n",
    "\"\"\")\n",
    "\n",
    "output_file = 'disease_related_UID.txt'\n",
    "with engine.connect() as conn:\n",
    "\n",
    "    result = pd.read_sql(query, conn)\n",
    "    \n",
    "\n",
    "    result.to_csv(output_file, \n",
    "                  sep='\\t', \n",
    "                  index=False, \n",
    "                  header=True,\n",
    "                  na_rep='N.A.')\n",
    "\n",
    "print(f\"exported {len(result)} records {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step4.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export Create a local mapping table for non verified by crispr lncRNA.\n",
    "query = text(\"\"\"\n",
    "    SELECT Lncbook_id,Noncode_id,UID\n",
    "    FROM esslnc \n",
    "    WHERE  vitro = 0;\n",
    "\"\"\")\n",
    "\n",
    "output_file = 'non_crispr_UID.txt'\n",
    "with engine.connect() as conn:\n",
    "\n",
    "    result = pd.read_sql(query, conn)\n",
    "    \n",
    "\n",
    "    result.to_csv(output_file, \n",
    "                  sep='\\t', \n",
    "                  index=False, \n",
    "                  header=True,\n",
    "                  na_rep='N.A.')\n",
    "\n",
    "print(f\"exported {len(result)} records {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4: Import transcript Entries\n",
    "The required files:\n",
    "1. /match/seq_splice.bed seq_delete.bed seq_casrx.bed seq_crispri.bed\n",
    "2. /match/esslnc2.fa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import Genomic location information.\n",
    "import pandas as pd\n",
    "\n",
    "# custum bed file, generated by gen_fa.ipynb step1, Just input different files as needed\n",
    "custum_bed_file = 'seq_casrx.bed'\n",
    "#transcript sequence crispr_gene.fa\n",
    "seq_file = 'lncRNAV2.fasta'\n",
    "\n",
    "mapping_df = pd.read_csv('crispr_mapping.tsv', sep='\\t',names=['UID','target'])\n",
    "target_to_uid = {}\n",
    "\n",
    "for _, row in mapping_df.iterrows():\n",
    "    target_to_uid[row['target']] = row['UID']\n",
    "\n",
    "df = pd.read_csv(custum_bed_file, sep='\\t', header=None, \n",
    "                 names=['chr','start','end','name','score','strand','block_starts','block_sizes'])\n",
    "\n",
    "\n",
    "def parse_fasta(fasta_file):\n",
    "    sequences = {}\n",
    "    current_seq_id = None\n",
    "    current_seq = []\n",
    "    \n",
    "    with open(fasta_file) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "\n",
    "                if current_seq_id:\n",
    "                    sequences[current_seq_id] = ''.join(current_seq)\n",
    "                current_seq_id = line[1:]  \n",
    "                current_seq = []\n",
    "            else:\n",
    "                current_seq.append(line)\n",
    "    if current_seq_id:\n",
    "        sequences[current_seq_id] = ''.join(current_seq)\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "\n",
    "transcript_fa = parse_fasta(seq_file)\n",
    "\n",
    "def process_transcript(row):\n",
    "\n",
    "    transcript_id = row['name'].rsplit('-',1)[1]\n",
    "    target = row['name'].rsplit('-',1)[0]\n",
    "    UID = target_to_uid.get(target)\n",
    "    fasta_seq = transcript_fa.get(transcript_id)\n",
    "    if fasta_seq is None:\n",
    "        FASTA = f\">{transcript_id}<br/>Sequence not found\"\n",
    "    else:\n",
    "        FASTA = f\">{transcript_id}<br/>{fasta_seq}\"\n",
    "\n",
    "    starts = [int(x) for x in row['block_starts'].split(',') if x]\n",
    "    sizes = [int(x) for x in row['block_sizes'].split(',') if x]\n",
    "    \n",
    "\n",
    "    exon_positions = []\n",
    "    for rel_start, size in zip(starts, sizes):\n",
    "        abs_start = row['start'] + rel_start + 1  # 1-base\n",
    "        abs_end = abs_start + size - 1\n",
    "        exon_positions.append(f\"{abs_start}-{abs_end}\")\n",
    "    \n",
    "    return {\n",
    "        'UID': UID,\n",
    "        'transcript_id': transcript_id,\n",
    "        'chr': row['chr'],\n",
    "        'start': row['start'] + 1,  # 1-base\n",
    "        'end': row['end'],\n",
    "        'length': sum(sizes),\n",
    "        'exon_num': len(sizes),\n",
    "        'exon_pos': ','.join(exon_positions),\n",
    "        'strand': row['strand'],\n",
    "        'FASTA':FASTA\n",
    "    }\n",
    "\n",
    "result_df = pd.DataFrame([process_transcript(row) for _, row in df.iterrows()])\n",
    "\n",
    "\n",
    "result_df.to_sql('trans', engine, if_exists='append', index=False)\n",
    "\n",
    "print(f\"Successfully import {len(result_df)} records to trans table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import non verified by crispr lncRNA \n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import text\n",
    "\n",
    "non_crispr_file = 'non_crispr_lncrna.csv'\n",
    "headers = pd.read_csv(non_crispr_file, nrows=0).columns.tolist()\n",
    "print(headers)\n",
    "\n",
    "sql = text(f\"\"\"\n",
    "    INSERT IGNORE INTO trans\n",
    "    ({', '.join(headers)})\n",
    "    VALUES ({', '.join([':' + col for col in headers])})\n",
    "\"\"\")\n",
    "\n",
    "total_rows = 0\n",
    "success_rows = 0\n",
    "\n",
    "for chunk in pd.read_csv(non_crispr_file, chunksize=1000):\n",
    "    with engine.connect() as conn:\n",
    "        for _, row in chunk.iterrows():\n",
    "            try:\n",
    "                result = conn.execute(sql, row.to_dict())\n",
    "                success_rows += result.rowcount\n",
    "                total_rows += 1\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "        conn.commit()\n",
    "print(f\"Total processed: {total_rows}\")\n",
    "print(f\"Successfully imported: {success_rows}\")\n",
    "print(f\"Skipped duplicates: {total_rows - success_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step5.3 Import sequence (lncRNA non verified by crispr)\n",
    "The required files(downloaded from lncbook v2.0 and Noncode V6.0)\n",
    "1. LncBookv2_OnlyLnc.fa\n",
    "2. outLncRNA.fa \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seq from lncbook/noncode\n",
    "import pandas as pd\n",
    "from sqlalchemy import text\n",
    "\n",
    "fa_file = 'LncBookv2_OnlyLnc.fa'\n",
    "\n",
    "sql_select = text(\"\"\"\n",
    "    SELECT transcript_id \n",
    "    FROM trans \n",
    "    WHERE FASTA IS NULL\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "fasta_dict = {}\n",
    "with open(fa_file, 'r') as f:\n",
    "    seq = ''\n",
    "    tid = ''\n",
    "    for line in f:\n",
    "        if line.startswith('>'):\n",
    "            if tid and seq:\n",
    "                fasta_dict[tid] = seq\n",
    "            header = line.strip().lstrip('>')\n",
    "            tid = header.split()[0]  # Use the first part of the header as transcript_id\n",
    "            seq = ''\n",
    "        else:\n",
    "            seq += line.strip()\n",
    "    if tid and seq:\n",
    "        fasta_dict[tid] = seq\n",
    "\n",
    "\n",
    "sql_update = text(\"\"\"\n",
    "    UPDATE trans \n",
    "    SET FASTA = :fasta , length = :length\n",
    "    WHERE transcript_id = :tid\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    transcript_ids = conn.execute(sql_select).fetchall()\n",
    "    print(f\"Found {len(transcript_ids)} transcripts without FASTA data.\")\n",
    "    for (tid,) in transcript_ids:\n",
    "        print(f\"Processing transcript ID: {tid}\")\n",
    "        if tid in fasta_dict:\n",
    "            fasta_content = f\"{tid}<br/>{fasta_dict[tid]}\"\n",
    "            length = len(fasta_dict[tid])\n",
    "            try:\n",
    "                print(f\"Updating {tid} with length {length}\")\n",
    "                conn.execute(sql_update, {\"fasta\": fasta_content, \"tid\": tid, \"length\": length})\n",
    "            except Exception as e:\n",
    "                print(f\"Error updating {tid}: {e}\")\n",
    "    \n",
    "    conn.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step5.4 import Mouse transcripts from dbesslnc\n",
    "The required files:\n",
    "1. /dbesslnc/mouse_trans.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mouse essential lncRNA data\n",
    "df = pd.read_csv('mouse_trans.csv', sep=',', usecols=lambda column: column != 'Name')\n",
    "\n",
    "for column in df.columns:\n",
    "    df[column] = df[column].astype(str).str.strip('\"')\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    df.to_sql('trans', \n",
    "              con=conn, \n",
    "              if_exists='append',\n",
    "              index=False)\n",
    "print(f\"successfully {len(df)} rows to trans table.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as conn:\n",
    "    # Read Noncode_id and UID where Organism is 'Mouse'\n",
    "    query = text(\"\"\"\n",
    "        SELECT Noncode_id, UID\n",
    "        FROM esslnc\n",
    "        WHERE Organism = 'Mouse'\n",
    "    \"\"\")\n",
    "    result = conn.execute(query)\n",
    "    rows = result.fetchall()\n",
    "\n",
    "    # Create a dictionary to map Noncode_id to UID\n",
    "    noncode_to_uid = {}\n",
    "    for row in rows:\n",
    "        noncode_id, uid = row\n",
    "        if noncode_id not in noncode_to_uid:\n",
    "            noncode_to_uid[noncode_id] = uid\n",
    "\n",
    "    # Update UID where Noncode_id is equal\n",
    "    for noncode_id, uid in noncode_to_uid.items():\n",
    "        update_sql = text(\"\"\"\n",
    "            UPDATE trans\n",
    "            SET UID = :uid\n",
    "            WHERE Organism = 'Mouse' AND Noncode_id = :noncode_id \n",
    "        \"\"\")\n",
    "        conn.execute(update_sql, {\"uid\": uid, \"noncode_id\": noncode_id})\n",
    "    \n",
    "    conn.commit()\n",
    "\n",
    "print(\"Update successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6: Import the mapped transcript IDs\n",
    "The required files\n",
    "1. /match/lncbook_map.tsv\n",
    "2. /match/noncode_map.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the mapped database gene IDs and transcript IDs,along with other mapped data.\n",
    "# You can modify SQL statements and input files to update fields in the database.\n",
    "map_file = 'lncbook_map.tsv' #*_map.tsv,\n",
    "\n",
    "map_df = pd.read_csv(map_file, sep='\\t', header=None)\n",
    "# filter\n",
    "map_dict = map_df[map_df[6] == 'transcript'][[1,3,4]].drop_duplicates()\n",
    "map_dict.columns = ['transcript_id','Lncbook_trans_id', 'Lncbook_id'] # You can modify the corresponding column names.\n",
    "# print(map_dict.head())\n",
    "\n",
    "\n",
    "for _, lnc_row in map_dict.iterrows():\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            update_sql = text(\"\"\"\n",
    "            UPDATE trans \n",
    "            SET Lncbook_trans_id = :Lncbook_trans_id, \n",
    "                Lncbook_id = :Lncbook_id\n",
    "            WHERE transcript_id = :transcript_id\n",
    "            \"\"\")\n",
    "            \n",
    "            result = conn.execute(update_sql, {\n",
    "                \"transcript_id\": lnc_row['transcript_id'],\n",
    "                \"Lncbook_id\": lnc_row['Lncbook_id'],\n",
    "                \"Lncbook_trans_id\": lnc_row['Lncbook_trans_id']\n",
    "            })\n",
    "            \n",
    "            rows_affected = result.rowcount\n",
    "            # print(f\"update records: {lnc_row['transcript_id']}, affected rows: {rows_affected}\")\n",
    "            \n",
    "            conn.commit()\n",
    "    except Exception as e:\n",
    "        print(f\"update fail: {lnc_row['transcript_id']}, error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step7: import CRISPR experiment record `/curated/exp_crispr.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "exp_df = pd.read_csv('../store/exp_crispr.csv')\n",
    "\n",
    "exp_df.to_sql('exp_crispr', \n",
    "              engine, \n",
    "              if_exists='append', \n",
    "              index=False,         \n",
    "              chunksize=1000)      \n",
    "\n",
    "print(f\"Successfully import {len(exp_df)} records to trans table.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step7.1 \n",
    "Group by   exp_type   and   target_id   columns, for groups with a count of 1, mark as 'cell-line specific' and update in the table; for groups with a count of 2-5, mark as 'common essential'; and for groups with a count greater than 5, mark as 'core essential'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import text\n",
    "\n",
    "\n",
    "sql_select = text(\"\"\"\n",
    "    SELECT exp_type, target_id \n",
    "    FROM exp_crispr\n",
    "\"\"\")\n",
    "\n",
    "with engine.connect() as conn:\n",
    "\n",
    "    df = pd.read_sql(sql_select, conn)\n",
    "    \n",
    "\n",
    "    group_counts = df.groupby(['exp_type', 'target_id']).size()\n",
    "    \n",
    "\n",
    "    cell_specific = group_counts[group_counts == 1].index\n",
    "    common = group_counts[(group_counts >= 2) & (group_counts <= 5)].index\n",
    "    core = group_counts[group_counts > 5].index\n",
    "    \n",
    "\n",
    "    update_sql = text(\"\"\"\n",
    "        UPDATE exp_crispr\n",
    "        SET role = :etype \n",
    "        WHERE exp_type = :exp AND target_id = :tid\n",
    "    \"\"\")\n",
    "    \n",
    "\n",
    "    for exp, tid in cell_specific:\n",
    "        conn.execute(update_sql, {\"etype\": \"cell-line specific\", \"exp\": exp, \"tid\": tid})\n",
    "        \n",
    "    for exp, tid in common:\n",
    "        conn.execute(update_sql, {\"etype\": \"common essential\", \"exp\": exp, \"tid\": tid})\n",
    "        \n",
    "    for exp, tid in core:\n",
    "        conn.execute(update_sql, {\"etype\": \"core essential\", \"exp\": exp, \"tid\": tid})\n",
    "    \n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step7.2: Assign generated UIDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "map_file = 'crispr_mapping.tsv' #*_map.tsv,\n",
    "\n",
    "map_df = pd.read_csv(map_file, sep='\\t', names=['UID','target'])\n",
    "# filter\n",
    "\n",
    "for _, lnc_row in map_df.iterrows():\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            update_sql = text(\"\"\"\n",
    "            UPDATE exp_crispr \n",
    "            SET UID= :UID\n",
    "            WHERE target_id = :target\n",
    "            \"\"\")\n",
    "            \n",
    "            result = conn.execute(update_sql, {\n",
    "                \"UID\": lnc_row['UID'],\n",
    "                \"target\": lnc_row['target'],\n",
    "            })\n",
    "            \n",
    "            rows_affected = result.rowcount            \n",
    "            conn.commit()\n",
    "    except Exception as e:\n",
    "        print(f\"update fail: {lnc_row['target']}, error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step7.3: Update the PUBMED ID of the esslnc table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the UID and PMID columns from exp_crispr,and update the PMID column in the esslnc table.\n",
    "query = text(\"\"\"\n",
    "    SELECT UID, PMID\n",
    "    FROM exp_crispr\n",
    "    WHERE PMID IS NOT NULL\n",
    "\"\"\")\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(query)\n",
    "    rows = result.fetchall()\n",
    "\n",
    "    for row in rows:\n",
    "        uid, pmid = row\n",
    "        update_sql = text(\"\"\"\n",
    "            UPDATE esslnc\n",
    "            SET PMID = CASE\n",
    "                WHEN PMID IS NULL THEN :pmid\n",
    "                ELSE CONCAT(PMID, ',', :pmid)\n",
    "            END\n",
    "            WHERE UID = :uid\n",
    "        \"\"\")\n",
    "        conn.execute(update_sql, {\"pmid\": pmid, \"uid\": uid})\n",
    "    \n",
    "    conn.commit()\n",
    "\n",
    "print(\"PMID updated successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step8：import variants Mapping table and variants\n",
    "The required files.\n",
    "1. /clinvar_map/db/construct_map/disease_mapping.csv\n",
    "2. /clinvar_map/db/construct_map/crispr_variant_mapping.csv\n",
    "3. /clinvar_map/db/crispr_overlap/variants_nocrispr.csv\n",
    "4. /clinvar_map/db/crispr_map/crispr_variants.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import variant information and mapping tables into the database.\n",
    "import pandas as pd\n",
    "from sqlalchemy import text\n",
    "\n",
    "variants_file = 'disease_mapping.csv'\n",
    "headers = pd.read_csv(variants_file, nrows=0).columns.tolist()\n",
    "\n",
    "#table name variants/lncrna_variant_mapping\n",
    "sql = text(f\"\"\"\n",
    "    INSERT IGNORE INTO lncrna_variant_mapping\n",
    "    ({', '.join(headers)})\n",
    "    VALUES ({', '.join([':' + col for col in headers])})\n",
    "\"\"\")\n",
    "\n",
    "total_rows = 0\n",
    "success_rows = 0\n",
    "\n",
    "for chunk in pd.read_csv(variants_file, chunksize=1000):\n",
    "    with engine.connect() as conn:\n",
    "        for _, row in chunk.iterrows():\n",
    "            try:\n",
    "                result = conn.execute(sql, row.to_dict())\n",
    "                success_rows += result.rowcount\n",
    "                total_rows += 1\n",
    "            except Exception as e:\n",
    "                continue\n",
    "        conn.commit()\n",
    "print(f\"Total processed: {total_rows}\")\n",
    "print(f\"Successfully imported: {success_rows}\")\n",
    "print(f\"Skipped duplicates: {total_rows - success_rows}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step8.2: Marking whether lncRNAs are associated with diseases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as conn:\n",
    "    # update esslnc table\n",
    "    update_sql = text(\"\"\"\n",
    "    UPDATE esslnc \n",
    "    SET disease_related = 1\n",
    "    WHERE UID IN (\n",
    "        SELECT DISTINCT UID \n",
    "        FROM lncrna_variant_mapping\n",
    "    )\n",
    "    \"\"\")\n",
    "    conn.execute(update_sql)\n",
    "    conn.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step9: import expression profile\n",
    "The required file\n",
    "1. /clinvar_map/db/exp/exp_profile.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all lncRNA expression profile\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import text\n",
    "\n",
    "expression_file = 'exp_profile.csv'\n",
    "headers = pd.read_csv(expression_file, nrows=0).columns.tolist()\n",
    "print(headers)\n",
    "\n",
    "sql = text(f\"\"\"\n",
    "    INSERT IGNORE INTO exp_profile\n",
    "    ({', '.join(headers)})\n",
    "    VALUES ({', '.join([':' + col for col in headers])})\n",
    "\"\"\")\n",
    "\n",
    "total_rows = 0\n",
    "success_rows = 0\n",
    "\n",
    "for chunk in pd.read_csv(expression_file, chunksize=1000):\n",
    "    with engine.connect() as conn:\n",
    "        for _, row in chunk.iterrows():\n",
    "            try:\n",
    "                result = conn.execute(sql, row.to_dict())\n",
    "                success_rows += result.rowcount\n",
    "                total_rows += 1\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "        conn.commit()\n",
    "print(f\"Total processed: {total_rows}\")\n",
    "print(f\"Successfully imported: {success_rows}\")\n",
    "print(f\"Skipped duplicates: {total_rows - success_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step10: All NULL entries in the database should be changed to N.A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as conn:\n",
    "    update_esslnc = text(\"\"\"\n",
    "    UPDATE esslnc \n",
    "    SET \n",
    "        NCBI_id = COALESCE(NCBI_id, 'N.A.'),\n",
    "        gene_name = COALESCE(gene_name, 'N.A.'),\n",
    "        Alias = COALESCE(Alias, 'N.A.'),\n",
    "        ensembl_id = COALESCE(ensembl_id, 'N.A.'),\n",
    "        Noncode_id = COALESCE(Noncode_id, 'N.A.'),\n",
    "        Lncbook_id = COALESCE(Lncbook_id, 'N.A.'),\n",
    "        reason = COALESCE(reason, 'N.A.'),\n",
    "        Go_annotation = COALESCE(Go_annotation, 'N.A.'),\n",
    "        target = COALESCE(target, 'N.A.')\n",
    "    \"\"\")\n",
    "    conn.execute(update_esslnc)\n",
    "  \n",
    "\n",
    "    # update_trans = text(\"\"\"\n",
    "    # UPDATE trans\n",
    "    # SET \n",
    "    #     Noncode_id = COALESCE(Noncode_id, 'N.A.'),\n",
    "    #     Noncode_trans_id = COALESCE(Noncode_trans_id, 'N.A.'),\n",
    "    #     Lncbook_id = COALESCE(Lncbook_id, 'N.A.'),\n",
    "    #     Lncbook_trans_id = COALESCE(Lncbook_trans_id, 'N.A.')\n",
    "    # \"\"\")\n",
    "    # conn.execute(update_trans)\n",
    "    # conn.commit()\n",
    "\n",
    "print(\"NULL2N.A. Update successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import Genomic location information.\n",
    "import pandas as pd\n",
    "\n",
    "# custum bed file, generated by gen_fa.ipynb step1, Just input different files as needed\n",
    "custum_bed_file = 'lncRNA_with_transcripts.csv'\n",
    "#transcript sequence crispr_gene.fa\n",
    "seq_file = 'lncRNAV2.fasta'\n",
    "\n",
    "# mapping_df = pd.read_csv('crispr_mapping.tsv', sep='\\t',names=['UID','target'])\n",
    "# target_to_uid = {}\n",
    "\n",
    "# for _, row in mapping_df.iterrows():\n",
    "#     target_to_uid[row['target']] = row['UID']\n",
    "\n",
    "df = pd.read_csv(custum_bed_file, sep=',')\n",
    "\n",
    "\n",
    "\n",
    "def process_transcript(row):\n",
    "    # print(row)\n",
    "    transcript_id = row['transcript_id']\n",
    "    UID = row['UID']\n",
    "    print(UID)\n",
    "    print(transcript_id)\n",
    "    with engine.connect() as conn:\n",
    "    # Update the trans table if transcript_id matches\n",
    "        update_sql = text(\"\"\"\n",
    "            UPDATE trans\n",
    "            SET UID = :UID\n",
    "            WHERE transcript_id = :transcript_id\n",
    "        \"\"\")\n",
    "        conn.execute(update_sql, {\"UID\": UID, \"transcript_id\": transcript_id})\n",
    "        conn.commit()\n",
    "\n",
    "result_df = pd.DataFrame([process_transcript(row) for _, row in df.iterrows()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the FA file used by the BLAST service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the FA file used by the BLAST service\n",
    "import pandas as pd\n",
    "from sqlalchemy import text\n",
    "\n",
    "# 查询数据库中的 transcript_id 和 FASTA 列\n",
    "query = text(\"\"\"\n",
    "    SELECT transcript_id, FASTA \n",
    "    FROM trans \n",
    "    WHERE FASTA IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "output_file = 'lncRNA2.fasta'\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(query)\n",
    "    rows = result.fetchall()\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        for transcript_id, fasta_content in rows:\n",
    "            if fasta_content and '<br/>' in fasta_content:\n",
    "                # 提取 <br/> 后面的序列部分\n",
    "                sequence = fasta_content.split('<br/>', 1)[1]\n",
    "                \n",
    "                # 写入标准 FASTA 格式\n",
    "                f.write(f\">{transcript_id}\\n\")\n",
    "                f.write(f\"{sequence}\\n\")\n",
    "            else:\n",
    "                print(f\"Warning: Invalid FASTA format for {transcript_id}\")\n",
    "\n",
    "print(f\"Successfully generated {output_file} with {len(rows)} sequences\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
