{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "# 1. 读取 CSV 文件\n",
    "csv_file_path = 'combined_all.csv'\n",
    "column_names = ['chr', 'start','end','gene_name','strand']\n",
    "df = pd.read_csv(csv_file_path, names=column_names, header=None)\n",
    "# # print(df.head(1))\n",
    "# df = df.dropna(subset=['exp_score'])\n",
    "# df['exp_score'] = df['exp_score'].astype(str)\n",
    "# df['exp_score'] = df['exp_score'].str.strip()\n",
    "# df = df[df['exp_score'] != '']\n",
    "# # df = df[df['exp_score'].notna() & (df['exp_score'] != '')]\n",
    "# print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 连接 MySQL 数据库\n",
    "# 假设使用 PyMySQL 作为驱动程序\n",
    "username = 'root'\n",
    "password = 'auggie'\n",
    "host = 'localhost'  # 通常是 'localhost'\n",
    "port = 3306  # 默认 MySQL 端口号\n",
    "database = 'dbess'\n",
    "engine = create_engine(f'mysql+pymysql://{username}:{password}@{host}:{port}/{database}')\n",
    "\n",
    "# # 3. 过滤掉第三列为空的行\n",
    "# df = df.dropna(subset=['exp_score'])\n",
    "\n",
    "# # 3. 添加其他固定字段\n",
    "# df['exp_type'] = 'CRISPR CasRx'\n",
    "# df['role'] = ''\n",
    "# df['cell_line'] = 'NICH460'\n",
    "\n",
    "\n",
    "\n",
    "# 6. 将数据存入数据库\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "df.to_sql('merge', con=engine, if_exists='append', index=False)\n",
    "\n",
    "session.commit()\n",
    "session.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取CSV文件\n",
    "df = pd.read_csv('test.csv')\n",
    "\n",
    "# 找到重复项\n",
    "duplicates = df[df.duplicated('name', keep=False)]\n",
    "\n",
    "# 只保留每组重复项中的一个\n",
    "unique_duplicates = duplicates.drop_duplicates(subset='name', keep='first')\n",
    "\n",
    "# 将结果保存到新的CSV文件\n",
    "unique_duplicates.to_csv('unique_duplicates.csv', index=False)\n",
    "\n",
    "print(\"已将有重复项的行保存到 'unique_duplicates.csv' 文件中。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 将csv转换为bed格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取CSV文件\n",
    "column_names = ['gene_id', 'chromosome', 'strand', 'start', 'end']\n",
    "df = pd.read_csv('casRX_gtf.csv',names=column_names)\n",
    "\n",
    "# 将start列减1以符合BED格式（BED格式的起始位置是从0开始）\n",
    "df['start'] = df['start'] - 1\n",
    "# 选择所需列的顺序，并添加必要的列\n",
    "df['chromosome'] = 'chr' + df['chromosome'].astype(str)\n",
    "bed_df = df[['chromosome', 'start', 'end', 'gene_id', 'strand']]\n",
    "bed_df.loc[:, 'score'] = 0  # BED文件中的第五列通常是score，这里设为0\n",
    "bed_df = bed_df[['chromosome', 'start', 'end', 'gene_id', 'score', 'strand']]\n",
    "\n",
    "# 将DataFrame保存为BED格式\n",
    "bed_df.to_csv('transcripts.bed', sep='\\t', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 旧版本genename 和 id 映射"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# 读取CSV文件\n",
    "csv_file_path = './map/sup.csv'\n",
    "column_names = ['gene', 'ensemble', 'gene_name']\n",
    "df = pd.read_csv(csv_file_path, names=column_names, header=None)\n",
    "\n",
    "# 数据库连接信息\n",
    "username = 'root'\n",
    "password = 'auggie'\n",
    "host = 'localhost'  # 通常是 'localhost'\n",
    "port = 3306  # 默认 MySQL 端口号\n",
    "database = 'dbess'\n",
    "engine = create_engine(f'mysql+pymysql://{username}:{password}@{host}:{port}/{database}')\n",
    "table_name = 'map_temp2'\n",
    "\n",
    "# 将DataFrame存入MySQL数据库\n",
    "df.to_sql(name=table_name, con=engine, if_exists='replace', index=False)\n",
    "\n",
    "print(\"数据已成功存入MySQL数据库中。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import text\n",
    "# 读取CSV文件\n",
    "csv_file_path = './map/map-lnc9.csv'\n",
    "column_names = ['lib_id', 'Lncbook_id']\n",
    "df = pd.read_csv(csv_file_path, names=column_names, header=None)\n",
    "print(df.head(10))\n",
    "\n",
    "# 数据库连接信息\n",
    "username = 'root'\n",
    "password = 'auggie'\n",
    "host = 'localhost'  # 通常是 'localhost'\n",
    "port = 3306  # 默认 MySQL 端口号\n",
    "database = 'dbess'\n",
    "table_name = 'merge'\n",
    "\n",
    "# 使用SQLAlchemy创建数据库引擎\n",
    "engine = create_engine(f'mysql+pymysql://{username}:{password}@{host}:{port}/{database}')\n",
    "\n",
    "# 将DataFrame存入MySQL数据库\n",
    "with engine.connect() as connection:\n",
    "    for index, row in df.iterrows():\n",
    "        # ens-map\n",
    "        # lib_id = row['lib_id']\n",
    "        # old_name = row['old_name']\n",
    "\n",
    "        # non-map\n",
    "        lib_id = row['lib_id']\n",
    "        Lncbook_id = row['Lncbook_id']\n",
    "        if pd.notna(lib_id):  # 仅在 old_name 非空时更新\n",
    "            query = text(f'''\n",
    "                UPDATE {table_name}\n",
    "                SET Lncbook_id = '{Lncbook_id}' \n",
    "                WHERE lib_id = '{lib_id}';\n",
    "            ''')\n",
    "            connection.execute(query, {'lib_id': lib_id, 'Lncbook_id': Lncbook_id})\n",
    "    \n",
    "    connection.commit()\n",
    "\n",
    "print(\"数据已成功更新到MySQL数据库中。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# 读取CSV文件的前两列\n",
    "csv_file_path = './map/merged_site2.csv'\n",
    "df = pd.read_csv(csv_file_path, usecols=[0, 1], names=['name', 'id'], header=None)\n",
    "\n",
    "# 数据库连接信息\n",
    "username = 'root'\n",
    "password = 'auggie'\n",
    "host = 'localhost'  # 通常是 'localhost'\n",
    "port = 3306  # 默认 MySQL 端口号\n",
    "database = 'dbess'\n",
    "table_name = 'merge'\n",
    "\n",
    "# 使用SQLAlchemy创建数据库引擎\n",
    "engine = create_engine(f'mysql+pymysql://{username}:{password}@{host}:{port}/{database}')\n",
    "\n",
    "# 将DataFrame存入MySQL数据库\n",
    "with engine.connect() as connection:\n",
    "    for index, row in df.iterrows():\n",
    "        name = row['name']\n",
    "        ensembl_id = row['id']\n",
    "        if pd.notna(ensembl_id):  # 仅在 ensembl_id 非空时更新\n",
    "            # 查找匹配的 gene_name\n",
    "            query = text(f'''\n",
    "                SELECT gene_name FROM {table_name}\n",
    "                WHERE gene_name LIKE CONCAT('%', :name, '%');\n",
    "            ''')\n",
    "            result = connection.execute(query, {'name': name}).fetchone()\n",
    "            if result:\n",
    "                # 更新 ensembl_id\n",
    "                update_query = text(f'''\n",
    "                    UPDATE {table_name}\n",
    "                    SET ensembl_id = :ensembl_id\n",
    "                    WHERE gene_name LIKE CONCAT('%', :name, '%');\n",
    "                ''')\n",
    "                connection.execute(update_query, {'ensembl_id': ensembl_id, 'name': name})\n",
    "    connection.commit()  # 提交事务\n",
    "\n",
    "print(\"数据已成功更新到MySQL数据库中。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取CSV文件\n",
    "csv_file_path = './map/update.csv'\n",
    "df = pd.read_csv(csv_file_path, names=['ensembl_id', 'gene_name', 'gene', 'ens_id'], header=None)\n",
    "\n",
    "# 遍历每一行，检查条件并打印\n",
    "for index, row in df.iterrows():\n",
    "    ensembl_id = row['ensembl_id']\n",
    "    ens_id = row['ens_id']\n",
    "    if pd.notna(ensembl_id) and ensembl_id != ens_id:\n",
    "        print(f\"ensembl_id: {ensembl_id}, gene_name: {row['gene_name']}, gene: {row['gene']}, ens_id: {ens_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# 读取文件\n",
    "file_path = './map/non-coding_RNA.txt'\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "# 选择指定的列\n",
    "columns_to_keep = ['hgnc_id', 'symbol', 'name', 'alias_symbol', 'prev_symbol', 'entrez_id', 'ensembl_gene_id', 'pubmed_id', 'lncipedia']\n",
    "df = df[columns_to_keep]\n",
    "print(df.head(10))\n",
    "# 数据库连接信息\n",
    "username = 'root'\n",
    "password = 'auggie'\n",
    "host = 'localhost'  # 通常是 'localhost'\n",
    "port = 3306  # 默认 MySQL 端口号\n",
    "database = 'dbess'\n",
    "table_name = 'map_name'\n",
    "\n",
    "# 使用SQLAlchemy创建数据库引擎\n",
    "engine = create_engine(f'mysql+pymysql://{username}:{password}@{host}:{port}/{database}')\n",
    "\n",
    "# 将DataFrame存入MySQL数据库\n",
    "df.to_sql(table_name, engine, if_exists='replace', index=False)\n",
    "\n",
    "print(\"数据已成功导入到MySQL数据库中。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 添加 文献原始的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# 读取文件\n",
    "file_path = './map/site-pair38.bed'\n",
    "df = pd.read_csv(file_path, sep='\\t', header=None, names=['chr', 'start', 'end', 'Lit_id', 'dot', 'strand'])\n",
    "\n",
    "# 选择指定的列并处理数据\n",
    "df['start'] = df['start'] + 1\n",
    "columns_to_keep = ['Lit_id', 'chr', 'strand', 'start', 'end']\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "# 数据库连接信息\n",
    "username = 'root'\n",
    "password = 'auggie'\n",
    "host = 'localhost'  # 通常是 'localhost'\n",
    "port = 3306  # 默认 MySQL 端口号\n",
    "database = 'dbess'\n",
    "table_name = 'crispr_lit'\n",
    "\n",
    "# 使用SQLAlchemy创建数据库引擎\n",
    "engine = create_engine(f'mysql+pymysql://{username}:{password}@{host}:{port}/{database}')\n",
    "\n",
    "# 将DataFrame存入MySQL数据库\n",
    "df.to_sql(table_name, engine, if_exists='append', index=False)\n",
    "\n",
    "print(\"数据已成功导入到MySQL数据库中。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# 读取文件\n",
    "file_path = './map/result.txt'\n",
    "df = pd.read_csv(file_path, sep='\\t', header=None, names=['NCBI_id','UID',\"Go_annotation\"])\n",
    "\n",
    "\n",
    "# 数据库连接信息\n",
    "username = 'root'\n",
    "password = 'auggie'\n",
    "host = 'localhost'  # 通常是 'localhost'\n",
    "port = 3306  # 默认 MySQL 端口号\n",
    "database = 'dbess'\n",
    "table_name = 'esslnc'\n",
    "\n",
    "# 使用SQLAlchemy创建数据库引擎\n",
    "engine = create_engine(f'mysql+pymysql://{username}:{password}@{host}:{port}/{database}')\n",
    "\n",
    "# 更新数据库中的数据\n",
    "# with engine.connect() as connection:\n",
    "#     for index, row in df.iterrows():\n",
    "#         lit_id = row['Lit_id']\n",
    "#         ensembl_id = row['ensembl_id']\n",
    "#         gene_name = row['gene_name']\n",
    "        \n",
    "#         # 更新语句\n",
    "#         update_query = text(f\"\"\"\n",
    "#             UPDATE {table_name}\n",
    "#             SET ensembl_id = :ensembl_id, gene_name = :gene_name\n",
    "#             WHERE Lit_id = :lit_id\n",
    "#         \"\"\")\n",
    "        \n",
    "#         # 执行更新\n",
    "#         connection.execute(update_query, {\"ensembl_id\": ensembl_id, \"gene_name\": gene_name, \"lit_id\": lit_id})\n",
    "\n",
    "#     connection.commit()    \n",
    "# print(\"数据已成功更新到MySQL数据库中。\")\n",
    "# max_id = 0\n",
    "# 更新ID列\n",
    "# 获取需要更新的行\n",
    "    # result = connection.execute(text(f\"SELECT * FROM {table_name} WHERE Organism = 'Mouse'\"))\n",
    "    # rows = result.fetchall()\n",
    "    \n",
    "    # # 执行更新\n",
    "    # for i, row in enumerate(rows, start=1):\n",
    "    #     new_id = max_id + i\n",
    "    #     new_id_str = f\"ELM{new_id:06d}\"\n",
    "    #     update_query = text(f\"\"\"\n",
    "    #         UPDATE {table_name}\n",
    "    #         SET ID = :new_id\n",
    "    #         WHERE Organism = 'Mouse' AND ID IS NULL\n",
    "    #         LIMIT 1\n",
    "    #     \"\"\")\n",
    "    #     connection.execute(update_query, {\"new_id\": new_id_str})\n",
    "with engine.connect() as connection:\n",
    "    \n",
    "    for index, rows in df.iterrows():\n",
    "        Go_annotation = rows['Go_annotation']\n",
    "        UID = rows['UID']\n",
    "        if pd.notna(Go_annotation):\n",
    "            update_query = text(f\"\"\"\n",
    "                UPDATE {table_name}\n",
    "                SET Go_annotation = :Go_annotation\n",
    "                WHERE UID = :UID\n",
    "            \"\"\")\n",
    "            connection.execute(update_query, {\"Go_annotation\": Go_annotation, \"UID\": UID})\n",
    "    connection.commit()\n",
    "\n",
    "print(\"数据已成功更新到MySQL数据库中。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 更新数据库数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建数据连接\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# 数据库连接信息\n",
    "username = 'root'\n",
    "password = 'auggie'\n",
    "host = 'localhost'  # 通常是 'localhost'\n",
    "port = 3306  # 默认 MySQL 端口号\n",
    "database = 'dbess'\n",
    "\n",
    "\n",
    "# 使用SQLAlchemy创建数据库引擎\n",
    "engine = create_engine(f'mysql+pymysql://{username}:{password}@{host}:{port}/{database}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV 文件已生成：output.csv\n"
     ]
    }
   ],
   "source": [
    "#更新数据库内容\n",
    "\n",
    "table_name = 'trans'\n",
    "\n",
    "# 读取文件\n",
    "# file_path = './map/score.csv'\n",
    "# df = pd.read_csv(file_path, sep=',')\n",
    "# print(df.head(10))\n",
    "# df = pd.read_csv(file_path, sep=',', header=None, names=['UID','chr','start','end','strand','source','Lncbook_id','Lncbook_trans_id'])\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    query = \"SELECT UID, variation_id FROM lncrna_variant_mapping\"\n",
    "    df = pd.read_sql(query, connection)\n",
    "    grouped_df = df.groupby('UID')['variation_id'].apply(lambda x: ';'.join(x.astype(str))).reset_index()\n",
    "\n",
    "    # 保存为 CSV 文件\n",
    "    grouped_df.to_csv('disease_related.csv', index=False)\n",
    "\n",
    "print(\"CSV 文件已生成：output.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取转录本信息\n",
    "import pandas as pd\n",
    "\n",
    "# 读取 UID.csv 文件的第一列信息\n",
    "uid_file_path = './map/UID.csv'\n",
    "uid_df = pd.read_csv(uid_file_path,names=['gene_id','UID'])\n",
    "gene_ids = uid_df['gene_id'].tolist()\n",
    "\n",
    "# 读取 GTF 文件并筛选出包含这些信息的行\n",
    "gtf_file_path = './map/gencode.v19.long_noncoding_RNAs.gtf'\n",
    "filtered_lines = []\n",
    "\n",
    "with open(gtf_file_path, 'r') as gtf_file:\n",
    "    for line in gtf_file:\n",
    "        if any(gene_id in line for gene_id in gene_ids):\n",
    "            filtered_lines.append(line)\n",
    "\n",
    "# 将筛选后的行保存到新的文件中\n",
    "output_file_path = './map/deletion.gtf'\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    output_file.writelines(filtered_lines)\n",
    "\n",
    "print(\"筛选后的行已成功保存到新的 GTF 文件中。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取 GTF 文件\n",
    "gtf_file_path = './map/deletion.gtf'\n",
    "gtf_df = pd.read_csv(gtf_file_path, sep='\\t', header=None, comment='#',\n",
    "                     names=['chrom', 'source', 'feature', 'start', 'end', 'score', 'strand', 'frame', 'attribute'])\n",
    "\n",
    "# 提取 gene_id 和 transcript_id\n",
    "def extract_attributes(attribute):\n",
    "    attributes = attribute.split(';')\n",
    "    gene_id = ''\n",
    "    transcript_id = ''\n",
    "    gene_name = ''\n",
    "    for attr in attributes:\n",
    "        if 'gene_id' in attr:\n",
    "            gene_id = attr.split('\"')[1]\n",
    "        elif 'transcript_id' in attr:\n",
    "            transcript_id = attr.split('\"')[1]\n",
    "        elif 'gene_name' in attr:\n",
    "            gene_name = attr.split('\"')[1]\n",
    "    return gene_id, transcript_id, gene_name\n",
    "\n",
    "gtf_df['gene_id'], gtf_df['transcript_id'], gtf_df['gene_name'] = zip(*gtf_df['attribute'].apply(extract_attributes))\n",
    "\n",
    "# # 创建 BED 文件的第四列\n",
    "# gtf_df['bed_name'] = gtf_df.apply(lambda row: f\"{row['gene_id']},{row['transcript_id']},{row['feature']},{row['gene_name']} \", axis=1)\n",
    "\n",
    "# # 创建 BED 文件\n",
    "# bed_df = gtf_df[['chrom', 'start', 'end', 'bed_name', 'score', 'strand']]\n",
    "\n",
    "# # 保存为 BED 文件\n",
    "bed_file_path = './map/deletion_map.bed'\n",
    "\n",
    "gtf_df[['transcript_id','gene_name']].to_csv(bed_file_path, sep='\\t', header=False, index=False)\n",
    "\n",
    "print(\"GTF 文件已成功转换为 BED 文件。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "input_file = './map/unfa.txt'\n",
    "output_file = './map/unfa1.txt'\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8', newline='') as outfile:\n",
    "    reader = csv.reader(infile, delimiter='\\t')\n",
    "    writer = csv.writer(outfile, delimiter='\\t')\n",
    "    \n",
    "    for row in reader:\n",
    "        # 删除前五列\n",
    "        new_row = row[5:]\n",
    "        writer.writerow(new_row)\n",
    "\n",
    "print(f\"前五列已从 {input_file} 中删除，并保存到 {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 序列截取\n",
    "import re\n",
    "\n",
    "# 读取txt文件中的位置信息\n",
    "def read_exon_positions(txt_file):\n",
    "    exon_positions = {}\n",
    "    id_map = {}\n",
    "    with open(txt_file, 'r', encoding='utf-8') as file:\n",
    "        next(file)  # 跳过第一行\n",
    "        for line in file:\n",
    "            parts = line.strip().split('\\t')\n",
    "            # print(parts)\n",
    "            if len(parts) > 1:\n",
    "                target_id = parts[3].strip()\n",
    "                transcript_id = parts[4].strip()\n",
    "                exon_pos = parts[-1]\n",
    "                exon_positions[transcript_id] = exon_pos.split(',')\n",
    "                id_map[transcript_id] = target_id\n",
    "    return exon_positions, id_map\n",
    "\n",
    "# 读取fa文件中的序列\n",
    "def read_fasta(fasta_file):\n",
    "    sequences = {}\n",
    "    with open(fasta_file, 'r', encoding='utf-8') as file:\n",
    "        seq_id = \"\"\n",
    "        seq = \"\"\n",
    "        strand = \"+\"\n",
    "        start_pos = 0\n",
    "        end_pos = 0\n",
    "        for line in file:\n",
    "            if line.startswith(\">ELH\"):\n",
    "                if seq_id:\n",
    "                    sequences[seq_id] = (seq, start_pos , end_pos, strand)\n",
    "                seq_id = line.strip().split()[0][1:]  # 去掉 '>' 并获取ID\n",
    "                seq = \"\"\n",
    "            elif line.startswith(\">chromosome\"):\n",
    "                parts = line.strip().split(':')\n",
    "                start_pos = int(parts[3])\n",
    "                end_pos = int(parts[4])\n",
    "                strand = \"-\" if parts[-1] == (\"-1\") else \"+\"\n",
    "                \n",
    "            else:\n",
    "                seq += line.strip()\n",
    "        if seq_id and seq:\n",
    "            sequences[seq_id] = (seq, start_pos, end_pos, strand)\n",
    "    return sequences\n",
    "\n",
    "# 根据位置信息截取并拼接序列\n",
    "def extract_and_concatenate_sequence(seq, positions, start_pos, end_pos, strand):\n",
    "    new_seq = \"\"\n",
    "    # print(strand)\n",
    "    for pos in positions:\n",
    "        start, end = map(int, pos.split('-'))\n",
    "        if start < start_pos or end > end_pos:\n",
    "            # print(f\"位置信息超出范围\",start,end)\n",
    "            continue\n",
    "        # print(start,end)\n",
    "        if strand == \"+\":\n",
    "            # 计算相对位置\n",
    "            relative_start = start - start_pos\n",
    "            relative_end = end - start_pos + 1\n",
    "            fragment = seq[relative_start:relative_end]\n",
    "            # print(relative_start,relative_end)\n",
    "            new_seq += fragment\n",
    "        else:\n",
    "            # 计算相对位置，从尾部开始\n",
    "            relative_start = end_pos - end\n",
    "            relative_end = end_pos - start + 1\n",
    "            fragment = seq[relative_start:relative_end]\n",
    "            new_seq = fragment + new_seq\n",
    "            # print(fragment)\n",
    "            \n",
    "    return new_seq\n",
    "\n",
    "def gen_fa(sequences, exon_positions, id_map, outfile):\n",
    "     with open(outfile, 'w', encoding='utf-8') as file:\n",
    "        for transcript_id, positions in exon_positions.items():\n",
    "            uid= id_map[transcript_id]\n",
    "            if uid in sequences:\n",
    "                seq, start_pos, end_pos, strand = sequences[uid]\n",
    "            \n",
    "                new_seq = extract_and_concatenate_sequence(seq, positions, start_pos, end_pos, strand)\n",
    "                if new_seq == \"\":\n",
    "                    print(f\"位置信息不正确\",transcript_id)\n",
    "                    continue\n",
    "                file.write(f\">{transcript_id}\\n\")\n",
    "                file.write(f\"{new_seq}\\n\")\n",
    "            else:\n",
    "                print(f\"未找到 {transcript_id} 的序列\")\n",
    "# 主函数\n",
    "def main():\n",
    "    txt_file = './map/unfa.txt'\n",
    "    fasta_file = './map/seq.fa'\n",
    "    outfile = './map/trans.fa'\n",
    "    trans_id = 'URS0000D5BD64_9606_2'\n",
    "\n",
    "    exon_positions,id_map = read_exon_positions(txt_file)\n",
    "\n",
    "    sequences = read_fasta(fasta_file)\n",
    "\n",
    "    for transcript_id, positions in exon_positions.items():\n",
    "        uid= id_map[transcript_id]\n",
    "        if uid in sequences:\n",
    "            seq, start_pos, end_pos, strand = sequences[uid]\n",
    "        \n",
    "            new_seq = extract_and_concatenate_sequence(seq, positions, start_pos, end_pos, strand)\n",
    "            if new_seq == \"\":\n",
    "                print(transcript_id)\n",
    "                continue\n",
    "        else:\n",
    "                print(f\"未找到 {transcript_id} 的序列\")\n",
    "    # 生成fa文件\n",
    "    # gen_fa(sequences, exon_positions, id_map, outfile)\n",
    "\n",
    "    \n",
    "    # test\n",
    "    # positions = exon_positions[trans_id]\n",
    "    # seq, start_pos, end_pos, strand = sequences[id_map[trans_id]]\n",
    "    # print(f\"序列ID\",id_map[trans_id])\n",
    "\n",
    "    # new_seq = extract_and_concatenate_sequence(seq, positions, start_pos, end_pos, strand)\n",
    "    # print(new_seq)\n",
    "\n",
    "    \n",
    "    # print(f\"拼接后的新序列:\\n{new_seq}\")\n",
    "    # print(f\"新序列长度: {len(new_seq)}\")\n",
    "    \n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更新数据库的fa文件\n",
    "# 创建数据连接\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# 数据库连接信息\n",
    "username = 'root'\n",
    "password = 'auggie'\n",
    "host = 'localhost'  # 通常是 'localhost'\n",
    "port = 3306  # 默认 MySQL 端口号\n",
    "database = 'dbess'\n",
    "\n",
    "\n",
    "# 使用SQLAlchemy创建数据库引擎\n",
    "engine = create_engine(f'mysql+pymysql://{username}:{password}@{host}:{port}/{database}')\n",
    "\n",
    "# 解析trans.fa文件\n",
    "def parse_fa_file(fa_file):\n",
    "    sequences = {}\n",
    "    with open(fa_file, 'r', encoding='utf-8') as file:\n",
    "        transcript_id = \"\"\n",
    "        seq = \"\"\n",
    "        for line in file:\n",
    "            if line.startswith(\">\"):\n",
    "                if transcript_id and seq:\n",
    "                    sequences[transcript_id] = seq\n",
    "                transcript_id = line.strip()[1:]  # 去掉 '>'\n",
    "                seq = \"\"\n",
    "            else:\n",
    "                seq += line.strip()\n",
    "        if transcript_id and seq:\n",
    "            sequences[transcript_id] = seq  # 添加最后一个序列\n",
    "    return sequences\n",
    "\n",
    "def update_database(fa_file, engine):\n",
    "    # 解析fa文件\n",
    "    sequences = parse_fa_file(fa_file)\n",
    "    \n",
    "    with engine.connect() as connection:\n",
    "    \n",
    "        for Lncbook_trans_id, sequence in sequences.items():\n",
    "            update_query = text(\"\"\"\n",
    "                UPDATE trans\n",
    "                SET FASTA = CONCAT('>', transcript_id, '|', Lncbook_trans_id, '<br/>', :sequence)\n",
    "                WHERE Lncbook_trans_id = :Lncbook_trans_id\n",
    "            \"\"\")\n",
    "            connection.execute(update_query, {\"sequence\": sequence, \"Lncbook_trans_id\": Lncbook_trans_id})\n",
    "        connection.commit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fa_file = './map/outLncRNA.fa'\n",
    "txt_file = './map/Non_trans.txt'\n",
    "df = pd.read_csv(txt_file, sep='\\t', header=0)\n",
    "trans = set(df['NONCODE_TRANSCRIPT_ID'])\n",
    "# print(trans)\n",
    "with engine.connect() as connection:\n",
    "    with open(fa_file, 'r', encoding='utf-8') as file:\n",
    "        NONCODE_TRANSCRIPT_ID = \"\"\n",
    "        seq = \"\"\n",
    "        for line in file:\n",
    "            if line.startswith(\">\"):\n",
    "                if NONCODE_TRANSCRIPT_ID and seq and NONCODE_TRANSCRIPT_ID in trans:\n",
    "                    update_query = text(\"\"\"\n",
    "                        UPDATE trans\n",
    "                        SET FASTA = CONCAT('>', transcript_id, '<br/>', :sequence)\n",
    "                        WHERE NONCODE_TRANSCRIPT_ID = :NONCODE_TRANSCRIPT_ID AND FASTA IS NULL\n",
    "                    \"\"\")\n",
    "                    connection.execute(update_query, {\"sequence\": seq, \"NONCODE_TRANSCRIPT_ID\": NONCODE_TRANSCRIPT_ID})\n",
    "                NONCODE_TRANSCRIPT_ID = line.strip()[1:]\n",
    "                seq = \"\"\n",
    "            else:\n",
    "                seq += line.strip()\n",
    "        if NONCODE_TRANSCRIPT_ID and seq and NONCODE_TRANSCRIPT_ID in trans:\n",
    "                update_query = text(\"\"\"\n",
    "                        UPDATE trans\n",
    "                        SET FASTA = CONCAT('>', transcript_id, '<br/>', :sequence)\n",
    "                        WHERE NONCODE_TRANSCRIPT_ID = :NONCODE_TRANSCRIPT_ID AND FASTA IS NULL\n",
    "                    \"\"\")\n",
    "                connection.execute(update_query, {\"sequence\": seq, \"NONCODE_TRANSCRIPT_ID\": NONCODE_TRANSCRIPT_ID})\n",
    "    connection.commit()\n",
    "    print(\"数据已成功更新到MySQL数据库中。\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_fa_file(fa_file):\n",
    "    sequences = {}\n",
    "    with open(fa_file, 'r', encoding='utf-8') as file:\n",
    "        transcript_id = \"\"\n",
    "        seq = \"\"\n",
    "        for line in file:\n",
    "            if line.startswith(\">\"):\n",
    "                if transcript_id and seq:\n",
    "                    sequences[transcript_id] = seq\n",
    "                transcript_id = line.strip()[1:]  # 去掉 '>'\n",
    "                seq = \"\"\n",
    "            else:\n",
    "                seq += line.strip()\n",
    "        if transcript_id and seq:\n",
    "            sequences[transcript_id] = seq  # 添加最后一个序列\n",
    "    return sequences\n",
    "\n",
    "fa_file = './map/LncBookv2_OnlyLnc.fa'\n",
    "seq = parse_fa_file(fa_file)\n",
    "print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 构建blast库\n",
    "# 读取TXT文件内容\n",
    "with open('./blast/lncrna/lncRNA2.txt', 'r', encoding='utf-8') as file:\n",
    "    txt_content = file.read()\n",
    "\n",
    "# 将 <br/> 替换为换行符\n",
    "fasta_content = txt_content.replace('<br/>', '\\n')\n",
    "\n",
    "\n",
    "\n",
    "# 将结果保存为FASTA文件\n",
    "with open('./blast/lncrna/lncRNA2.fasta', 'w', encoding='utf-8') as file:\n",
    "    file.write(fasta_content)\n",
    "\n",
    "print(\"转换完成，结果已保存为 output.fasta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# 读取数据文件\n",
    "boxplot_data = pd.read_csv('boxplot_data.txt', sep='\\t', header=None, names=['Value'])  # 全部数据\n",
    "highlight_data = pd.read_csv('highlight.txt', sep='\\t', header=None, names=['Value'])  # 需要标注的点\n",
    "\n",
    "# 添加标记列，标记特殊点\n",
    "boxplot_data['IsHighlight'] = boxplot_data['Value'].isin(highlight_data['Value'])\n",
    "\n",
    "# 设置颜色映射\n",
    "colors = boxplot_data['IsHighlight'].map({True: 'red', False: 'gray'})\n",
    "\n",
    "# 绘制箱线图\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(y='Value', data=boxplot_data, showfliers=False, width=0.3)\n",
    "\n",
    "# # 绘制普通点\n",
    "# sns.stripplot(y='Value', data=boxplot_data[~boxplot_data['IsHighlight']], \n",
    "#               color='gray', alpha=0.6, jitter=True)\n",
    "\n",
    "# 绘制特殊点\n",
    "sns.stripplot(y='Value', data=boxplot_data[boxplot_data['IsHighlight']], \n",
    "              color='red', alpha=0.8, size=8, label='Highlight Points')\n",
    "\n",
    "# 设置标题和图例\n",
    "plt.legend(title=\"Point Type\", loc='upper right')\n",
    "plt.title(\"Boxplot with Highlighted Points (No Category)\", fontsize=14)\n",
    "plt.ylabel(\"Value\", fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
