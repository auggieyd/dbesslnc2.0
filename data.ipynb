{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "# 1. 读取 CSV 文件\n",
    "csv_file_path = 'combined_all.csv'\n",
    "column_names = ['chr', 'start','end','gene_name','strand']\n",
    "df = pd.read_csv(csv_file_path, names=column_names, header=None)\n",
    "# # print(df.head(1))\n",
    "# df = df.dropna(subset=['exp_score'])\n",
    "# df['exp_score'] = df['exp_score'].astype(str)\n",
    "# df['exp_score'] = df['exp_score'].str.strip()\n",
    "# df = df[df['exp_score'] != '']\n",
    "# # df = df[df['exp_score'].notna() & (df['exp_score'] != '')]\n",
    "# print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 连接 MySQL 数据库\n",
    "# 假设使用 PyMySQL 作为驱动程序\n",
    "username = 'root'\n",
    "password = 'auggie'\n",
    "host = 'localhost'  # 通常是 'localhost'\n",
    "port = 3306  # 默认 MySQL 端口号\n",
    "database = 'dbess'\n",
    "engine = create_engine(f'mysql+pymysql://{username}:{password}@{host}:{port}/{database}')\n",
    "\n",
    "# # 3. 过滤掉第三列为空的行\n",
    "# df = df.dropna(subset=['exp_score'])\n",
    "\n",
    "# # 3. 添加其他固定字段\n",
    "# df['exp_type'] = 'CRISPR CasRx'\n",
    "# df['role'] = ''\n",
    "# df['cell_line'] = 'NICH460'\n",
    "\n",
    "\n",
    "\n",
    "# 6. 将数据存入数据库\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "df.to_sql('merge', con=engine, if_exists='append', index=False)\n",
    "\n",
    "session.commit()\n",
    "session.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已将有重复项的行保存到 'unique_duplicates.csv' 文件中。\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取CSV文件\n",
    "df = pd.read_csv('test.csv')\n",
    "\n",
    "# 找到重复项\n",
    "duplicates = df[df.duplicated('name', keep=False)]\n",
    "\n",
    "# 只保留每组重复项中的一个\n",
    "unique_duplicates = duplicates.drop_duplicates(subset='name', keep='first')\n",
    "\n",
    "# 将结果保存到新的CSV文件\n",
    "unique_duplicates.to_csv('unique_duplicates.csv', index=False)\n",
    "\n",
    "print(\"已将有重复项的行保存到 'unique_duplicates.csv' 文件中。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 将csv转换为bed格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\auggi\\AppData\\Local\\Temp\\ipykernel_18500\\438118684.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bed_df.loc[:, 'score'] = 0  # BED文件中的第五列通常是score，这里设为0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取CSV文件\n",
    "column_names = ['gene_id', 'chromosome', 'strand', 'start', 'end']\n",
    "df = pd.read_csv('casRX_gtf.csv',names=column_names)\n",
    "\n",
    "# 将start列减1以符合BED格式（BED格式的起始位置是从0开始）\n",
    "df['start'] = df['start'] - 1\n",
    "# 选择所需列的顺序，并添加必要的列\n",
    "df['chromosome'] = 'chr' + df['chromosome'].astype(str)\n",
    "bed_df = df[['chromosome', 'start', 'end', 'gene_id', 'strand']]\n",
    "bed_df.loc[:, 'score'] = 0  # BED文件中的第五列通常是score，这里设为0\n",
    "bed_df = bed_df[['chromosome', 'start', 'end', 'gene_id', 'score', 'strand']]\n",
    "\n",
    "# 将DataFrame保存为BED格式\n",
    "bed_df.to_csv('transcripts.bed', sep='\\t', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 旧版本genename 和 id 映射"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据已成功存入MySQL数据库中。\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# 读取CSV文件\n",
    "csv_file_path = './map/sup.csv'\n",
    "column_names = ['gene', 'ensemble', 'gene_name']\n",
    "df = pd.read_csv(csv_file_path, names=column_names, header=None)\n",
    "\n",
    "# 数据库连接信息\n",
    "username = 'root'\n",
    "password = 'auggie'\n",
    "host = 'localhost'  # 通常是 'localhost'\n",
    "port = 3306  # 默认 MySQL 端口号\n",
    "database = 'dbess'\n",
    "engine = create_engine(f'mysql+pymysql://{username}:{password}@{host}:{port}/{database}')\n",
    "table_name = 'map_temp2'\n",
    "\n",
    "# 将DataFrame存入MySQL数据库\n",
    "df.to_sql(name=table_name, con=engine, if_exists='replace', index=False)\n",
    "\n",
    "print(\"数据已成功存入MySQL数据库中。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     lib_id     Lncbook_id\n",
      "0                   LH03019  HSALNG0086779\n",
      "1                   LH17009  HSALNG0139958\n",
      "2           XXbac-B444P24.8  HSALNG0134112\n",
      "3  human_lncrna_fused_31188  HSALNG0108875\n",
      "4                AC067968.3  HSALNG0126410\n",
      "5                   LH05235  HSALNG0105247\n",
      "6  human_lncrna_fused_71239  HSALNG0043513\n",
      "7              RP11-49K24.4  HSALNG0121372\n",
      "8                   LH08637  HSALNG0013804\n",
      "9                   LH10987  HSALNG0026314\n",
      "数据已成功更新到MySQL数据库中。\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import text\n",
    "# 读取CSV文件\n",
    "csv_file_path = './map/map-lnc9.csv'\n",
    "column_names = ['lib_id', 'Lncbook_id']\n",
    "df = pd.read_csv(csv_file_path, names=column_names, header=None)\n",
    "print(df.head(10))\n",
    "\n",
    "# 数据库连接信息\n",
    "username = 'root'\n",
    "password = 'auggie'\n",
    "host = 'localhost'  # 通常是 'localhost'\n",
    "port = 3306  # 默认 MySQL 端口号\n",
    "database = 'dbess'\n",
    "table_name = 'merge'\n",
    "\n",
    "# 使用SQLAlchemy创建数据库引擎\n",
    "engine = create_engine(f'mysql+pymysql://{username}:{password}@{host}:{port}/{database}')\n",
    "\n",
    "# 将DataFrame存入MySQL数据库\n",
    "with engine.connect() as connection:\n",
    "    for index, row in df.iterrows():\n",
    "        # ens-map\n",
    "        # lib_id = row['lib_id']\n",
    "        # old_name = row['old_name']\n",
    "\n",
    "        # non-map\n",
    "        lib_id = row['lib_id']\n",
    "        Lncbook_id = row['Lncbook_id']\n",
    "        if pd.notna(lib_id):  # 仅在 old_name 非空时更新\n",
    "            query = text(f'''\n",
    "                UPDATE {table_name}\n",
    "                SET Lncbook_id = '{Lncbook_id}' \n",
    "                WHERE lib_id = '{lib_id}';\n",
    "            ''')\n",
    "            connection.execute(query, {'lib_id': lib_id, 'Lncbook_id': Lncbook_id})\n",
    "    \n",
    "    connection.commit()\n",
    "\n",
    "print(\"数据已成功更新到MySQL数据库中。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据已成功更新到MySQL数据库中。\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# 读取CSV文件的前两列\n",
    "csv_file_path = './map/merged_site2.csv'\n",
    "df = pd.read_csv(csv_file_path, usecols=[0, 1], names=['name', 'id'], header=None)\n",
    "\n",
    "# 数据库连接信息\n",
    "username = 'root'\n",
    "password = 'auggie'\n",
    "host = 'localhost'  # 通常是 'localhost'\n",
    "port = 3306  # 默认 MySQL 端口号\n",
    "database = 'dbess'\n",
    "table_name = 'merge'\n",
    "\n",
    "# 使用SQLAlchemy创建数据库引擎\n",
    "engine = create_engine(f'mysql+pymysql://{username}:{password}@{host}:{port}/{database}')\n",
    "\n",
    "# 将DataFrame存入MySQL数据库\n",
    "with engine.connect() as connection:\n",
    "    for index, row in df.iterrows():\n",
    "        name = row['name']\n",
    "        ensembl_id = row['id']\n",
    "        if pd.notna(ensembl_id):  # 仅在 ensembl_id 非空时更新\n",
    "            # 查找匹配的 gene_name\n",
    "            query = text(f'''\n",
    "                SELECT gene_name FROM {table_name}\n",
    "                WHERE gene_name LIKE CONCAT('%', :name, '%');\n",
    "            ''')\n",
    "            result = connection.execute(query, {'name': name}).fetchone()\n",
    "            if result:\n",
    "                # 更新 ensembl_id\n",
    "                update_query = text(f'''\n",
    "                    UPDATE {table_name}\n",
    "                    SET ensembl_id = :ensembl_id\n",
    "                    WHERE gene_name LIKE CONCAT('%', :name, '%');\n",
    "                ''')\n",
    "                connection.execute(update_query, {'ensembl_id': ensembl_id, 'name': name})\n",
    "    connection.commit()  # 提交事务\n",
    "\n",
    "print(\"数据已成功更新到MySQL数据库中。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ensembl_id: ensembl_id, gene_name: gene_name, gene: gene, ens_id: ens_id\n",
      "ensembl_id: ENSG00000215908, gene_name: CROCCP3, gene: CROCCP2, ens_id: ENSG00000291072\n",
      "ensembl_id: ENSG00000237950, gene_name: GNG12-AS1, gene: RP11-7O11.3, ens_id: ENSG00000288573\n",
      "ensembl_id: ENSG00000224167, gene_name: SLC16A1-AS1, gene: RP3-522D1.1, ens_id: ENSG00000226419\n",
      "ensembl_id: ENSG00000231429, gene_name: LINC02798, gene: RP11-343N15.2, ens_id: ENSG00000227082\n",
      "ensembl_id: ENSG00000215859, gene_name: PDZK1P1, gene: PDZK1P2, ens_id: ENSG00000290705\n",
      "ensembl_id: ENSG00000231196, gene_name: LINC01138, gene: RP11-495P10.8, ens_id: ENSG00000274020\n",
      "ensembl_id: ENSG00000234232, gene_name: LINC00869, gene: RP11-353N4.5, ens_id: ENSG00000290790\n",
      "ensembl_id: ENSG00000224739, gene_name: LINC01819, gene: AC016735.1, ens_id: ENSG00000231826\n",
      "ensembl_id: ENSG00000229352, gene_name: TESHL, gene: AC007563.3, ens_id: ENSG00000223874\n",
      "ensembl_id: ENSG00000226155, gene_name: TNK2-AS1, gene: AC124944.3, ens_id: ENSG00000224614\n",
      "ensembl_id: ENSG00000253798, gene_name: NBPF22P, gene: AC008694.3, ens_id: ENSG00000290601\n",
      "ensembl_id: ENSG00000230914, gene_name: KIF19BP, gene: AC004840.8, ens_id: ENSG00000293436\n",
      "ensembl_id: ENSG00000230435, gene_name: COX19, gene: AC004160.4, ens_id: ENSG00000230333\n",
      "ensembl_id: ENSG00000225969, gene_name: EPHB6, gene: LINC00035, ens_id: ENSG00000293503\n",
      "ensembl_id: ENSG00000253929, gene_name: PCAT1, gene: CASC21, ens_id: ENSG00000253438\n",
      "ensembl_id: ENSG00000233776, gene_name: UBE2R2-AS1, gene: LINC01251, ens_id: ENSG00000235481\n",
      "ensembl_id: ENSG00000277778, gene_name: PGM5P2, gene: PGM5P2, ens_id: ENSG00000290923\n",
      "ensembl_id: ENSG00000228426, gene_name: ZNF32-AS3, gene: RP11-402L1.11, ens_id: ENSG00000223910\n",
      "ensembl_id: ENSG00000254452, gene_name: RAB1B, gene: RP11-867G23.4, ens_id: ENSG00000245156\n",
      "ensembl_id: ENSG00000255479, gene_name: LINC02757, gene: RP11-672A2.6, ens_id: ENSG00000255363\n",
      "ensembl_id: ENSG00000254761, gene_name: OR4A44P, gene: RP11-672A2.1, ens_id: ENSG00000290785\n",
      "ensembl_id: ENSG00000250770, gene_name: TAS2R64P, gene: RP5-1063M23.1, ens_id: ENSG00000291189\n",
      "ensembl_id: ENSG00000258612, gene_name: RPSAP3, gene: RP11-671J11.6, ens_id: ENSG00000259002\n",
      "ensembl_id: ENSG00000196364, gene_name: TPSD1, gene: PRSS29P, ens_id: ENSG00000290756\n",
      "ensembl_id: ENSG00000260517, gene_name: RRN3P2, gene: RP11-426C22.5, ens_id: ENSG00000291188\n",
      "ensembl_id: ENSG00000260750, gene_name: ZCCHC14-DT, gene: RP11-482M8.1, ens_id: ENSG00000288568\n",
      "ensembl_id: ENSG00000232698, gene_name: MIR6070, gene: AP001058.3, ens_id: ENSG00000232124\n",
      "ensembl_id: ENSG00000100181, gene_name: TPTEP1, gene: TPTEP1, ens_id: ENSG00000290418\n",
      "ensembl_id: ENSG00000237517, gene_name: DGCR5, gene: DGCR5, ens_id: ENSG00000273032\n",
      "ensembl_id: ENSG00000226287, gene_name: TMEM191A, gene: TMEM191A, ens_id: ENSG00000291085\n",
      "ensembl_id: ENSG00000236850, gene_name: TOP3BP1, gene: BMS1P20, ens_id: ENSG00000286129\n",
      "ensembl_id: ENSG00000234626, gene_name: SLC5A1, gene: RP1-149A16.12, ens_id: ENSG00000289873\n",
      "ensembl_id: ENSG00000247844, gene_name: CASC19, gene: CCAT1;LH15738, ens_id: ENSG00000254166\n",
      "ensembl_id: ENSG00000247844, gene_name: CASC19, gene: CCAT1;LH15741, ens_id: ENSG00000254166\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取CSV文件\n",
    "csv_file_path = './map/update.csv'\n",
    "df = pd.read_csv(csv_file_path, names=['ensembl_id', 'gene_name', 'gene', 'ens_id'], header=None)\n",
    "\n",
    "# 遍历每一行，检查条件并打印\n",
    "for index, row in df.iterrows():\n",
    "    ensembl_id = row['ensembl_id']\n",
    "    ens_id = row['ens_id']\n",
    "    if pd.notna(ensembl_id) and ensembl_id != ens_id:\n",
    "        print(f\"ensembl_id: {ensembl_id}, gene_name: {row['gene_name']}, gene: {row['gene']}, ens_id: {ens_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      hgnc_id       symbol                                               name  \\\n",
      "0  HGNC:37133     A1BG-AS1                               A1BG antisense RNA 1   \n",
      "1  HGNC:27057      A2M-AS1                                A2M antisense RNA 1   \n",
      "2  HGNC:41022    A2ML1-AS1                              A2ML1 antisense RNA 1   \n",
      "3  HGNC:41523    A2ML1-AS2                              A2ML1 antisense RNA 2   \n",
      "4  HGNC:50301  AADACL2-AS1                            AADACL2 antisense RNA 1   \n",
      "5  HGNC:51526        AATBC  apoptosis associated transcript in bladder cancer   \n",
      "6  HGNC:49667       ABALON     apoptotic BCL2L1-antisense long non-coding RNA   \n",
      "7  HGNC:39983    ABCA9-AS1                              ABCA9 antisense RNA 1   \n",
      "8  HGNC:40055    ABCC5-AS1                              ABCC5 antisense RNA 1   \n",
      "9  HGNC:53628     ABCF1-DT                         ABCF1 divergent transcript   \n",
      "\n",
      "  alias_symbol                prev_symbol    entrez_id  ensembl_gene_id  \\\n",
      "0     FLJ23569  NCRNA00181|A1BGAS|A1BG-AS     503538.0  ENSG00000268895   \n",
      "1          NaN                        NaN     144571.0  ENSG00000245105   \n",
      "2          NaN                        NaN  100874108.0  ENSG00000256661   \n",
      "3          NaN                        NaN  106478979.0  ENSG00000256904   \n",
      "4          NaN                        NaN  101928142.0  ENSG00000242908   \n",
      "5          NaN                        NaN     284837.0  ENSG00000215458   \n",
      "6         INXS                        NaN  103021294.0  ENSG00000281376   \n",
      "7          NaN                        NaN  104355297.0  ENSG00000231749   \n",
      "8          NaN                        NaN  100873982.0  ENSG00000223882   \n",
      "9          NaN                        NaN  107986587.0              NaN   \n",
      "\n",
      "  pubmed_id    lncipedia  \n",
      "0       NaN     A1BG-AS1  \n",
      "1       NaN      A2M-AS1  \n",
      "2       NaN    A2ML1-AS1  \n",
      "3       NaN    A2ML1-AS2  \n",
      "4       NaN  AADACL2-AS1  \n",
      "5  25473900        AATBC  \n",
      "6       NaN          NaN  \n",
      "7       NaN    ABCA9-AS1  \n",
      "8       NaN    ABCC5-AS1  \n",
      "9       NaN          NaN  \n",
      "数据已成功导入到MySQL数据库中。\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# 读取文件\n",
    "file_path = './map/non-coding_RNA.txt'\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "# 选择指定的列\n",
    "columns_to_keep = ['hgnc_id', 'symbol', 'name', 'alias_symbol', 'prev_symbol', 'entrez_id', 'ensembl_gene_id', 'pubmed_id', 'lncipedia']\n",
    "df = df[columns_to_keep]\n",
    "print(df.head(10))\n",
    "# 数据库连接信息\n",
    "username = 'root'\n",
    "password = 'auggie'\n",
    "host = 'localhost'  # 通常是 'localhost'\n",
    "port = 3306  # 默认 MySQL 端口号\n",
    "database = 'dbess'\n",
    "table_name = 'map_name'\n",
    "\n",
    "# 使用SQLAlchemy创建数据库引擎\n",
    "engine = create_engine(f'mysql+pymysql://{username}:{password}@{host}:{port}/{database}')\n",
    "\n",
    "# 将DataFrame存入MySQL数据库\n",
    "df.to_sql(table_name, engine, if_exists='replace', index=False)\n",
    "\n",
    "print(\"数据已成功导入到MySQL数据库中。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 添加 文献原始的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据已成功导入到MySQL数据库中。\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# 读取文件\n",
    "file_path = './map/site-pair38.bed'\n",
    "df = pd.read_csv(file_path, sep='\\t', header=None, names=['chr', 'start', 'end', 'Lit_id', 'dot', 'strand'])\n",
    "\n",
    "# 选择指定的列并处理数据\n",
    "df['start'] = df['start'] + 1\n",
    "columns_to_keep = ['Lit_id', 'chr', 'strand', 'start', 'end']\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "# 数据库连接信息\n",
    "username = 'root'\n",
    "password = 'auggie'\n",
    "host = 'localhost'  # 通常是 'localhost'\n",
    "port = 3306  # 默认 MySQL 端口号\n",
    "database = 'dbess'\n",
    "table_name = 'crispr_lit'\n",
    "\n",
    "# 使用SQLAlchemy创建数据库引擎\n",
    "engine = create_engine(f'mysql+pymysql://{username}:{password}@{host}:{port}/{database}')\n",
    "\n",
    "# 将DataFrame存入MySQL数据库\n",
    "df.to_sql(table_name, engine, if_exists='append', index=False)\n",
    "\n",
    "print(\"数据已成功导入到MySQL数据库中。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据已成功更新到MySQL数据库中。\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# 读取文件\n",
    "file_path = './map/unlnc-lncbook.csv'\n",
    "df = pd.read_csv(file_path, sep=',', header=None, names=['ID','Lncbook_id'])\n",
    "\n",
    "\n",
    "# 数据库连接信息\n",
    "username = 'root'\n",
    "password = 'auggie'\n",
    "host = 'localhost'  # 通常是 'localhost'\n",
    "port = 3306  # 默认 MySQL 端口号\n",
    "database = 'dbess'\n",
    "table_name = 'esslnc'\n",
    "\n",
    "# 使用SQLAlchemy创建数据库引擎\n",
    "engine = create_engine(f'mysql+pymysql://{username}:{password}@{host}:{port}/{database}')\n",
    "\n",
    "# 更新数据库中的数据\n",
    "# with engine.connect() as connection:\n",
    "#     for index, row in df.iterrows():\n",
    "#         lit_id = row['Lit_id']\n",
    "#         ensembl_id = row['ensembl_id']\n",
    "#         gene_name = row['gene_name']\n",
    "        \n",
    "#         # 更新语句\n",
    "#         update_query = text(f\"\"\"\n",
    "#             UPDATE {table_name}\n",
    "#             SET ensembl_id = :ensembl_id, gene_name = :gene_name\n",
    "#             WHERE Lit_id = :lit_id\n",
    "#         \"\"\")\n",
    "        \n",
    "#         # 执行更新\n",
    "#         connection.execute(update_query, {\"ensembl_id\": ensembl_id, \"gene_name\": gene_name, \"lit_id\": lit_id})\n",
    "\n",
    "#     connection.commit()    \n",
    "# print(\"数据已成功更新到MySQL数据库中。\")\n",
    "# max_id = 0\n",
    "# 更新ID列\n",
    "# 获取需要更新的行\n",
    "    # result = connection.execute(text(f\"SELECT * FROM {table_name} WHERE Organism = 'Mouse'\"))\n",
    "    # rows = result.fetchall()\n",
    "    \n",
    "    # # 执行更新\n",
    "    # for i, row in enumerate(rows, start=1):\n",
    "    #     new_id = max_id + i\n",
    "    #     new_id_str = f\"ELM{new_id:06d}\"\n",
    "    #     update_query = text(f\"\"\"\n",
    "    #         UPDATE {table_name}\n",
    "    #         SET ID = :new_id\n",
    "    #         WHERE Organism = 'Mouse' AND ID IS NULL\n",
    "    #         LIMIT 1\n",
    "    #     \"\"\")\n",
    "    #     connection.execute(update_query, {\"new_id\": new_id_str})\n",
    "with engine.connect() as connection:\n",
    "    \n",
    "    for index, rows in df.iterrows():\n",
    "        Lncbook_id = rows['Lncbook_id']\n",
    "        ID = rows['ID']\n",
    "        if pd.notna(Lncbook_id):\n",
    "            update_query = text(f\"\"\"\n",
    "                UPDATE {table_name}\n",
    "                SET Lncbook_id = :Lncbook_id\n",
    "                WHERE ID = :ID\n",
    "            \"\"\")\n",
    "            connection.execute(update_query, {\"Lncbook_id\": Lncbook_id, \"ID\": ID})\n",
    "    connection.commit()\n",
    "\n",
    "print(\"数据已成功更新到MySQL数据库中。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 更新数据库数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建数据连接\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# 数据库连接信息\n",
    "username = 'root'\n",
    "password = 'auggie'\n",
    "host = 'localhost'  # 通常是 'localhost'\n",
    "port = 3306  # 默认 MySQL 端口号\n",
    "database = 'dbess'\n",
    "\n",
    "\n",
    "# 使用SQLAlchemy创建数据库引擎\n",
    "engine = create_engine(f'mysql+pymysql://{username}:{password}@{host}:{port}/{database}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据已成功更新到MySQL数据库中。\n"
     ]
    }
   ],
   "source": [
    "#更新数据库内容\n",
    "\n",
    "table_name = 'trans'\n",
    "\n",
    "# 读取文件\n",
    "file_path = './map/trans_lnc_have.tsv'\n",
    "df = pd.read_csv(file_path, sep='\\t', header=None, names=['UID','chr','start','end','strand','source','Lncbook_id','Lncbook_trans_id'])\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    \n",
    "    for index, rows in df.iterrows():\n",
    "        Lncbook_id = rows['Lncbook_id']\n",
    "        UID = rows['UID']\n",
    "        Lncbook_trans_id = rows['Lncbook_trans_id']\n",
    "\n",
    "        if pd.notna(Lncbook_id):\n",
    "            insert_query = text(f\"\"\"\n",
    "                INSERT INTO {table_name} (UID, Lncbook_id, Lncbook_trans_id)\n",
    "                VALUES (:UID, :Lncbook_id, :Lncbook_trans_id)\n",
    "            \"\"\")\n",
    "            connection.execute(insert_query, {\"Lncbook_id\": Lncbook_id, \"UID\": ID,\"Lncbook_trans_id\": Lncbook_trans_id})\n",
    "    connection.commit()\n",
    "\n",
    "print(\"数据已成功更新到MySQL数据库中。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "筛选后的行已成功保存到新的 GTF 文件中。\n"
     ]
    }
   ],
   "source": [
    "# 获取转录本信息\n",
    "import pandas as pd\n",
    "\n",
    "# 读取 UID.csv 文件的第一列信息\n",
    "uid_file_path = './map/UID.csv'\n",
    "uid_df = pd.read_csv(uid_file_path,names=['gene_id','UID'])\n",
    "gene_ids = uid_df['gene_id'].tolist()\n",
    "\n",
    "# 读取 GTF 文件并筛选出包含这些信息的行\n",
    "gtf_file_path = './map/gencode.v19.long_noncoding_RNAs.gtf'\n",
    "filtered_lines = []\n",
    "\n",
    "with open(gtf_file_path, 'r') as gtf_file:\n",
    "    for line in gtf_file:\n",
    "        if any(gene_id in line for gene_id in gene_ids):\n",
    "            filtered_lines.append(line)\n",
    "\n",
    "# 将筛选后的行保存到新的文件中\n",
    "output_file_path = './map/deletion.gtf'\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    output_file.writelines(filtered_lines)\n",
    "\n",
    "print(\"筛选后的行已成功保存到新的 GTF 文件中。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GTF 文件已成功转换为 BED 文件。\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取 GTF 文件\n",
    "gtf_file_path = './map/deletion.gtf'\n",
    "gtf_df = pd.read_csv(gtf_file_path, sep='\\t', header=None, comment='#',\n",
    "                     names=['chrom', 'source', 'feature', 'start', 'end', 'score', 'strand', 'frame', 'attribute'])\n",
    "\n",
    "# 提取 gene_id 和 transcript_id\n",
    "def extract_attributes(attribute):\n",
    "    attributes = attribute.split(';')\n",
    "    gene_id = ''\n",
    "    transcript_id = ''\n",
    "    gene_name = ''\n",
    "    for attr in attributes:\n",
    "        if 'gene_id' in attr:\n",
    "            gene_id = attr.split('\"')[1]\n",
    "        elif 'transcript_id' in attr:\n",
    "            transcript_id = attr.split('\"')[1]\n",
    "        elif 'gene_name' in attr:\n",
    "            gene_name = attr.split('\"')[1]\n",
    "    return gene_id, transcript_id, gene_name\n",
    "\n",
    "gtf_df['gene_id'], gtf_df['transcript_id'], gtf_df['gene_name'] = zip(*gtf_df['attribute'].apply(extract_attributes))\n",
    "\n",
    "# # 创建 BED 文件的第四列\n",
    "# gtf_df['bed_name'] = gtf_df.apply(lambda row: f\"{row['gene_id']},{row['transcript_id']},{row['feature']},{row['gene_name']} \", axis=1)\n",
    "\n",
    "# # 创建 BED 文件\n",
    "# bed_df = gtf_df[['chrom', 'start', 'end', 'bed_name', 'score', 'strand']]\n",
    "\n",
    "# # 保存为 BED 文件\n",
    "bed_file_path = './map/deletion_map.bed'\n",
    "\n",
    "gtf_df[['transcript_id','gene_name']].to_csv(bed_file_path, sep='\\t', header=False, index=False)\n",
    "\n",
    "print(\"GTF 文件已成功转换为 BED 文件。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前五列已从 ./map/unfa.txt 中删除，并保存到 ./map/unfa1.txt\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "input_file = './map/unfa.txt'\n",
    "output_file = './map/unfa1.txt'\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8', newline='') as outfile:\n",
    "    reader = csv.reader(infile, delimiter='\\t')\n",
    "    writer = csv.writer(outfile, delimiter='\\t')\n",
    "    \n",
    "    for row in reader:\n",
    "        # 删除前五列\n",
    "        new_row = row[5:]\n",
    "        writer.writerow(new_row)\n",
    "\n",
    "print(f\"前五列已从 {input_file} 中删除，并保存到 {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 序列截取\n",
    "import re\n",
    "\n",
    "# 读取txt文件中的位置信息\n",
    "def read_exon_positions(txt_file):\n",
    "    exon_positions = {}\n",
    "    id_map = {}\n",
    "    with open(txt_file, 'r', encoding='utf-8') as file:\n",
    "        next(file)  # 跳过第一行\n",
    "        for line in file:\n",
    "            parts = line.strip().split('\\t')\n",
    "            # print(parts)\n",
    "            if len(parts) > 1:\n",
    "                target_id = parts[3].strip()\n",
    "                transcript_id = parts[4].strip()\n",
    "                exon_pos = parts[-1]\n",
    "                exon_positions[transcript_id] = exon_pos.split(',')\n",
    "                id_map[transcript_id] = target_id\n",
    "    return exon_positions, id_map\n",
    "\n",
    "# 读取fa文件中的序列\n",
    "def read_fasta(fasta_file):\n",
    "    sequences = {}\n",
    "    with open(fasta_file, 'r', encoding='utf-8') as file:\n",
    "        seq_id = \"\"\n",
    "        seq = \"\"\n",
    "        strand = \"+\"\n",
    "        start_pos = 0\n",
    "        end_pos = 0\n",
    "        for line in file:\n",
    "            if line.startswith(\">ELH\"):\n",
    "                if seq_id:\n",
    "                    sequences[seq_id] = (seq, start_pos , end_pos, strand)\n",
    "                seq_id = line.strip().split()[0][1:]  # 去掉 '>' 并获取ID\n",
    "                seq = \"\"\n",
    "            elif line.startswith(\">chromosome\"):\n",
    "                parts = line.strip().split(':')\n",
    "                start_pos = int(parts[3])\n",
    "                end_pos = int(parts[4])\n",
    "                strand = \"-\" if parts[-1] == (\"-1\") else \"+\"\n",
    "                \n",
    "            else:\n",
    "                seq += line.strip()\n",
    "        if seq_id and seq:\n",
    "            sequences[seq_id] = (seq, start_pos, end_pos, strand)\n",
    "    return sequences\n",
    "\n",
    "# 根据位置信息截取并拼接序列\n",
    "def extract_and_concatenate_sequence(seq, positions, start_pos, end_pos, strand):\n",
    "    new_seq = \"\"\n",
    "    # print(strand)\n",
    "    for pos in positions:\n",
    "        start, end = map(int, pos.split('-'))\n",
    "        if start < start_pos or end > end_pos:\n",
    "            # print(f\"位置信息超出范围\",start,end)\n",
    "            continue\n",
    "        # print(start,end)\n",
    "        if strand == \"+\":\n",
    "            # 计算相对位置\n",
    "            relative_start = start - start_pos\n",
    "            relative_end = end - start_pos + 1\n",
    "            fragment = seq[relative_start:relative_end]\n",
    "            # print(relative_start,relative_end)\n",
    "            new_seq += fragment\n",
    "        else:\n",
    "            # 计算相对位置，从尾部开始\n",
    "            relative_start = end_pos - end\n",
    "            relative_end = end_pos - start + 1\n",
    "            fragment = seq[relative_start:relative_end]\n",
    "            new_seq = fragment + new_seq\n",
    "            # print(fragment)\n",
    "            \n",
    "    return new_seq\n",
    "\n",
    "def gen_fa(sequences, exon_positions, id_map, outfile):\n",
    "     with open(outfile, 'w', encoding='utf-8') as file:\n",
    "        for transcript_id, positions in exon_positions.items():\n",
    "            uid= id_map[transcript_id]\n",
    "            if uid in sequences:\n",
    "                seq, start_pos, end_pos, strand = sequences[uid]\n",
    "            \n",
    "                new_seq = extract_and_concatenate_sequence(seq, positions, start_pos, end_pos, strand)\n",
    "                if new_seq == \"\":\n",
    "                    print(f\"位置信息不正确\",transcript_id)\n",
    "                    continue\n",
    "                file.write(f\">{transcript_id}\\n\")\n",
    "                file.write(f\"{new_seq}\\n\")\n",
    "            else:\n",
    "                print(f\"未找到 {transcript_id} 的序列\")\n",
    "# 主函数\n",
    "def main():\n",
    "    txt_file = './map/unfa.txt'\n",
    "    fasta_file = './map/seq.fa'\n",
    "    outfile = './map/trans.fa'\n",
    "    trans_id = 'URS0000D5BD64_9606_2'\n",
    "\n",
    "    exon_positions,id_map = read_exon_positions(txt_file)\n",
    "\n",
    "    sequences = read_fasta(fasta_file)\n",
    "\n",
    "    for transcript_id, positions in exon_positions.items():\n",
    "        uid= id_map[transcript_id]\n",
    "        if uid in sequences:\n",
    "            seq, start_pos, end_pos, strand = sequences[uid]\n",
    "        \n",
    "            new_seq = extract_and_concatenate_sequence(seq, positions, start_pos, end_pos, strand)\n",
    "            if new_seq == \"\":\n",
    "                print(transcript_id)\n",
    "                continue\n",
    "        else:\n",
    "                print(f\"未找到 {transcript_id} 的序列\")\n",
    "    # 生成fa文件\n",
    "    # gen_fa(sequences, exon_positions, id_map, outfile)\n",
    "\n",
    "    \n",
    "    # test\n",
    "    # positions = exon_positions[trans_id]\n",
    "    # seq, start_pos, end_pos, strand = sequences[id_map[trans_id]]\n",
    "    # print(f\"序列ID\",id_map[trans_id])\n",
    "\n",
    "    # new_seq = extract_and_concatenate_sequence(seq, positions, start_pos, end_pos, strand)\n",
    "    # print(new_seq)\n",
    "\n",
    "    \n",
    "    # print(f\"拼接后的新序列:\\n{new_seq}\")\n",
    "    # print(f\"新序列长度: {len(new_seq)}\")\n",
    "    \n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据已成功更新到MySQL数据库中。\n"
     ]
    }
   ],
   "source": [
    "# 更新数据库的fa文件\n",
    "# 创建数据连接\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# 数据库连接信息\n",
    "username = 'root'\n",
    "password = 'auggie'\n",
    "host = 'localhost'  # 通常是 'localhost'\n",
    "port = 3306  # 默认 MySQL 端口号\n",
    "database = 'dbess'\n",
    "\n",
    "\n",
    "# 使用SQLAlchemy创建数据库引擎\n",
    "engine = create_engine(f'mysql+pymysql://{username}:{password}@{host}:{port}/{database}')\n",
    "\n",
    "# 解析trans.fa文件\n",
    "def parse_fa_file(fa_file):\n",
    "    sequences = {}\n",
    "    with open(fa_file, 'r', encoding='utf-8') as file:\n",
    "        transcript_id = \"\"\n",
    "        seq = \"\"\n",
    "        for line in file:\n",
    "            if line.startswith(\">\"):\n",
    "                if transcript_id and seq:\n",
    "                    sequences[transcript_id] = seq\n",
    "                transcript_id = line.strip()[1:]  # 去掉 '>'\n",
    "                seq = \"\"\n",
    "            else:\n",
    "                seq += line.strip()\n",
    "        if transcript_id and seq:\n",
    "            sequences[transcript_id] = seq  # 添加最后一个序列\n",
    "    return sequences\n",
    "\n",
    "def update_database(fa_file, engine):\n",
    "    # 解析fa文件\n",
    "    sequences = parse_fa_file(fa_file)\n",
    "    \n",
    "    with engine.connect() as connection:\n",
    "    \n",
    "        for Lncbook_trans_id, sequence in sequences.items():\n",
    "            update_query = text(\"\"\"\n",
    "                UPDATE trans\n",
    "                SET FASTA = CONCAT('>', transcript_id, '|', Lncbook_trans_id, '<br/>', :sequence)\n",
    "                WHERE Lncbook_trans_id = :Lncbook_trans_id\n",
    "            \"\"\")\n",
    "            connection.execute(update_query, {\"sequence\": sequence, \"Lncbook_trans_id\": Lncbook_trans_id})\n",
    "        connection.commit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fa_file = './map/outLncRNA.fa'\n",
    "txt_file = './map/Non_trans.txt'\n",
    "df = pd.read_csv(txt_file, sep='\\t', header=0)\n",
    "trans = set(df['NONCODE_TRANSCRIPT_ID'])\n",
    "# print(trans)\n",
    "with engine.connect() as connection:\n",
    "    with open(fa_file, 'r', encoding='utf-8') as file:\n",
    "        NONCODE_TRANSCRIPT_ID = \"\"\n",
    "        seq = \"\"\n",
    "        for line in file:\n",
    "            if line.startswith(\">\"):\n",
    "                if NONCODE_TRANSCRIPT_ID and seq and NONCODE_TRANSCRIPT_ID in trans:\n",
    "                    update_query = text(\"\"\"\n",
    "                        UPDATE trans\n",
    "                        SET FASTA = CONCAT('>', transcript_id, '<br/>', :sequence)\n",
    "                        WHERE NONCODE_TRANSCRIPT_ID = :NONCODE_TRANSCRIPT_ID AND FASTA IS NULL\n",
    "                    \"\"\")\n",
    "                    connection.execute(update_query, {\"sequence\": seq, \"NONCODE_TRANSCRIPT_ID\": NONCODE_TRANSCRIPT_ID})\n",
    "                NONCODE_TRANSCRIPT_ID = line.strip()[1:]\n",
    "                seq = \"\"\n",
    "            else:\n",
    "                seq += line.strip()\n",
    "        if NONCODE_TRANSCRIPT_ID and seq and NONCODE_TRANSCRIPT_ID in trans:\n",
    "                update_query = text(\"\"\"\n",
    "                        UPDATE trans\n",
    "                        SET FASTA = CONCAT('>', transcript_id, '<br/>', :sequence)\n",
    "                        WHERE NONCODE_TRANSCRIPT_ID = :NONCODE_TRANSCRIPT_ID AND FASTA IS NULL\n",
    "                    \"\"\")\n",
    "                connection.execute(update_query, {\"sequence\": seq, \"NONCODE_TRANSCRIPT_ID\": NONCODE_TRANSCRIPT_ID})\n",
    "    connection.commit()\n",
    "    print(\"数据已成功更新到MySQL数据库中。\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_fa_file(fa_file):\n",
    "    sequences = {}\n",
    "    with open(fa_file, 'r', encoding='utf-8') as file:\n",
    "        transcript_id = \"\"\n",
    "        seq = \"\"\n",
    "        for line in file:\n",
    "            if line.startswith(\">\"):\n",
    "                if transcript_id and seq:\n",
    "                    sequences[transcript_id] = seq\n",
    "                transcript_id = line.strip()[1:]  # 去掉 '>'\n",
    "                seq = \"\"\n",
    "            else:\n",
    "                seq += line.strip()\n",
    "        if transcript_id and seq:\n",
    "            sequences[transcript_id] = seq  # 添加最后一个序列\n",
    "    return sequences\n",
    "\n",
    "fa_file = './map/LncBookv2_OnlyLnc.fa'\n",
    "seq = parse_fa_file(fa_file)\n",
    "print(seq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
